{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Insurance Claims- Fraud Detection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import scipy as stats\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "Importing the dataset\n",
    "# Reading the csv file from dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/dsrscientist/Data-Science-ML-Capstone-Projects/master/Automobile_insurance_fraud.csv\")\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df\n",
    "months_as_customer\tage\tpolicy_number\tpolicy_bind_date\tpolicy_state\tpolicy_csl\tpolicy_deductable\tpolicy_annual_premium\tumbrella_limit\tinsured_zip\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tcapital-gains\tcapital-loss\tincident_date\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tincident_location\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tproperty_damage\tbodily_injuries\twitnesses\tpolice_report_available\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tauto_make\tauto_model\tauto_year\tfraud_reported\t_c39\n",
    "0\t328\t48\t521585\t17-10-2014\tOH\t250/500\t1000\t1406.91\t0\t466132\tMALE\tMD\tcraft-repair\tsleeping\thusband\t53300\t0\t25-01-2015\tSingle Vehicle Collision\tSide Collision\tMajor Damage\tPolice\tSC\tColumbus\t9935 4th Drive\t5\t1\tYES\t1\t2\tYES\t71610\t6510\t13020\t52080\tSaab\t92x\t2004\tY\tNaN\n",
    "1\t228\t42\t342868\t27-06-2006\tIN\t250/500\t2000\t1197.22\t5000000\t468176\tMALE\tMD\tmachine-op-inspct\treading\tother-relative\t0\t0\t21-01-2015\tVehicle Theft\t?\tMinor Damage\tPolice\tVA\tRiverwood\t6608 MLK Hwy\t8\t1\t?\t0\t0\t?\t5070\t780\t780\t3510\tMercedes\tE400\t2007\tY\tNaN\n",
    "2\t134\t29\t687698\t06-09-2000\tOH\t100/300\t2000\t1413.14\t5000000\t430632\tFEMALE\tPhD\tsales\tboard-games\town-child\t35100\t0\t22-02-2015\tMulti-vehicle Collision\tRear Collision\tMinor Damage\tPolice\tNY\tColumbus\t7121 Francis Lane\t7\t3\tNO\t2\t3\tNO\t34650\t7700\t3850\t23100\tDodge\tRAM\t2007\tN\tNaN\n",
    "3\t256\t41\t227811\t25-05-1990\tIL\t250/500\t2000\t1415.74\t6000000\t608117\tFEMALE\tPhD\tarmed-forces\tboard-games\tunmarried\t48900\t-62400\t10-01-2015\tSingle Vehicle Collision\tFront Collision\tMajor Damage\tPolice\tOH\tArlington\t6956 Maple Drive\t5\t1\t?\t1\t2\tNO\t63400\t6340\t6340\t50720\tChevrolet\tTahoe\t2014\tY\tNaN\n",
    "4\t228\t44\t367455\t06-06-2014\tIL\t500/1000\t1000\t1583.91\t6000000\t610706\tMALE\tAssociate\tsales\tboard-games\tunmarried\t66000\t-46000\t17-02-2015\tVehicle Theft\t?\tMinor Damage\tNone\tNY\tArlington\t3041 3rd Ave\t20\t1\tNO\t0\t1\tNO\t6500\t1300\t650\t4550\tAccura\tRSX\t2009\tN\tNaN\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "995\t3\t38\t941851\t16-07-1991\tOH\t500/1000\t1000\t1310.80\t0\t431289\tFEMALE\tMasters\tcraft-repair\tpaintball\tunmarried\t0\t0\t22-02-2015\tSingle Vehicle Collision\tFront Collision\tMinor Damage\tFire\tNC\tNorthbrook\t6045 Andromedia St\t20\t1\tYES\t0\t1\t?\t87200\t17440\t8720\t61040\tHonda\tAccord\t2006\tN\tNaN\n",
    "996\t285\t41\t186934\t05-01-2014\tIL\t100/300\t1000\t1436.79\t0\t608177\tFEMALE\tPhD\tprof-specialty\tsleeping\twife\t70900\t0\t24-01-2015\tSingle Vehicle Collision\tRear Collision\tMajor Damage\tFire\tSC\tNorthbend\t3092 Texas Drive\t23\t1\tYES\t2\t3\t?\t108480\t18080\t18080\t72320\tVolkswagen\tPassat\t2015\tN\tNaN\n",
    "997\t130\t34\t918516\t17-02-2003\tOH\t250/500\t500\t1383.49\t3000000\t442797\tFEMALE\tMasters\tarmed-forces\tbungie-jumping\tother-relative\t35100\t0\t23-01-2015\tMulti-vehicle Collision\tSide Collision\tMinor Damage\tPolice\tNC\tArlington\t7629 5th St\t4\t3\t?\t2\t3\tYES\t67500\t7500\t7500\t52500\tSuburu\tImpreza\t1996\tN\tNaN\n",
    "998\t458\t62\t533940\t18-11-2011\tIL\t500/1000\t2000\t1356.92\t5000000\t441714\tMALE\tAssociate\thandlers-cleaners\tbase-jumping\twife\t0\t0\t26-02-2015\tSingle Vehicle Collision\tRear Collision\tMajor Damage\tOther\tNY\tArlington\t6128 Elm Lane\t2\t1\t?\t0\t1\tYES\t46980\t5220\t5220\t36540\tAudi\tA5\t1998\tN\tNaN\n",
    "999\t456\t60\t556080\t11-11-1996\tOH\t250/500\t1000\t766.19\t0\t612260\tFEMALE\tAssociate\tsales\tkayaking\thusband\t0\t0\t26-02-2015\tParked Car\t?\tMinor Damage\tPolice\tWV\tColumbus\t1416 Cherokee Ridge\t6\t1\t?\t0\t3\t?\t5060\t460\t920\t3680\tMercedes\tE400\t2007\tN\tNaN\n",
    "1000 rows Ã— 40 columns\n",
    "\n",
    "Here we have imported the data using pd.read_csv.The dataset contains the details of the insurance policy along with the customer details. It also has the details of the accident on the basis of which the claims have been made.\n",
    "\n",
    "The dataset contains both categorical and numerical columns. Here \"fraud_reported\"is our traget column, since it has two categories so it termed to be \"Classification Problem\"where we need predict if an insurance claim is fraudulent or not.\n",
    "\n",
    "Exploratory Data Analysis (EDA)\n",
    "# Checking dimension of dataset\n",
    "df.shape\n",
    "(1000, 40)\n",
    "The dataset contains 1000 rows and 40 columns. Out of40 columns 39 are independent columns and remaining one is our target variable \"fraud_reported\".\n",
    "\n",
    "# To get good overview of the dataset\n",
    "df.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1000 entries, 0 to 999\n",
    "Data columns (total 40 columns):\n",
    " #   Column                       Non-Null Count  Dtype  \n",
    "---  ------                       --------------  -----  \n",
    " 0   months_as_customer           1000 non-null   int64  \n",
    " 1   age                          1000 non-null   int64  \n",
    " 2   policy_number                1000 non-null   int64  \n",
    " 3   policy_bind_date             1000 non-null   object \n",
    " 4   policy_state                 1000 non-null   object \n",
    " 5   policy_csl                   1000 non-null   object \n",
    " 6   policy_deductable            1000 non-null   int64  \n",
    " 7   policy_annual_premium        1000 non-null   float64\n",
    " 8   umbrella_limit               1000 non-null   int64  \n",
    " 9   insured_zip                  1000 non-null   int64  \n",
    " 10  insured_sex                  1000 non-null   object \n",
    " 11  insured_education_level      1000 non-null   object \n",
    " 12  insured_occupation           1000 non-null   object \n",
    " 13  insured_hobbies              1000 non-null   object \n",
    " 14  insured_relationship         1000 non-null   object \n",
    " 15  capital-gains                1000 non-null   int64  \n",
    " 16  capital-loss                 1000 non-null   int64  \n",
    " 17  incident_date                1000 non-null   object \n",
    " 18  incident_type                1000 non-null   object \n",
    " 19  collision_type               1000 non-null   object \n",
    " 20  incident_severity            1000 non-null   object \n",
    " 21  authorities_contacted        1000 non-null   object \n",
    " 22  incident_state               1000 non-null   object \n",
    " 23  incident_city                1000 non-null   object \n",
    " 24  incident_location            1000 non-null   object \n",
    " 25  incident_hour_of_the_day     1000 non-null   int64  \n",
    " 26  number_of_vehicles_involved  1000 non-null   int64  \n",
    " 27  property_damage              1000 non-null   object \n",
    " 28  bodily_injuries              1000 non-null   int64  \n",
    " 29  witnesses                    1000 non-null   int64  \n",
    " 30  police_report_available      1000 non-null   object \n",
    " 31  total_claim_amount           1000 non-null   int64  \n",
    " 32  injury_claim                 1000 non-null   int64  \n",
    " 33  property_claim               1000 non-null   int64  \n",
    " 34  vehicle_claim                1000 non-null   int64  \n",
    " 35  auto_make                    1000 non-null   object \n",
    " 36  auto_model                   1000 non-null   object \n",
    " 37  auto_year                    1000 non-null   int64  \n",
    " 38  fraud_reported               1000 non-null   object \n",
    " 39  _c39                         0 non-null      float64\n",
    "dtypes: float64(2), int64(17), object(21)\n",
    "memory usage: 312.6+ KB\n",
    "This gives the information about the dataset which includes indexing type, column type, non-null values and memory usage.\n",
    "\n",
    "Here the column _c39 has 0 non null values which means it has one unique values throughout the data so we can drop this column.\n",
    "\n",
    "# Dropping _c39 column\n",
    "df.drop(\"_c39\",axis=1,inplace=True)\n",
    "# Checking null values\n",
    "df.isnull().sum()\n",
    "months_as_customer             0\n",
    "age                            0\n",
    "policy_number                  0\n",
    "policy_bind_date               0\n",
    "policy_state                   0\n",
    "policy_csl                     0\n",
    "policy_deductable              0\n",
    "policy_annual_premium          0\n",
    "umbrella_limit                 0\n",
    "insured_zip                    0\n",
    "insured_sex                    0\n",
    "insured_education_level        0\n",
    "insured_occupation             0\n",
    "insured_hobbies                0\n",
    "insured_relationship           0\n",
    "capital-gains                  0\n",
    "capital-loss                   0\n",
    "incident_date                  0\n",
    "incident_type                  0\n",
    "collision_type                 0\n",
    "incident_severity              0\n",
    "authorities_contacted          0\n",
    "incident_state                 0\n",
    "incident_city                  0\n",
    "incident_location              0\n",
    "incident_hour_of_the_day       0\n",
    "number_of_vehicles_involved    0\n",
    "property_damage                0\n",
    "bodily_injuries                0\n",
    "witnesses                      0\n",
    "police_report_available        0\n",
    "total_claim_amount             0\n",
    "injury_claim                   0\n",
    "property_claim                 0\n",
    "vehicle_claim                  0\n",
    "auto_make                      0\n",
    "auto_model                     0\n",
    "auto_year                      0\n",
    "fraud_reported                 0\n",
    "dtype: int64\n",
    "None of the columns in the dataset contains any null values.\n",
    "\n",
    "# Let's visualize the null values clearly\n",
    "sns.heatmap(df.isnull(), cmap=\"gist_earth\")\n",
    "plt.show()\n",
    "\n",
    "We can clearly observe that there are no null values present in the data.\n",
    "\n",
    "# Checking the type of dataset\n",
    "df.dtypes\n",
    "months_as_customer               int64\n",
    "age                              int64\n",
    "policy_number                    int64\n",
    "policy_bind_date                object\n",
    "policy_state                    object\n",
    "policy_csl                      object\n",
    "policy_deductable                int64\n",
    "policy_annual_premium          float64\n",
    "umbrella_limit                   int64\n",
    "insured_zip                      int64\n",
    "insured_sex                     object\n",
    "insured_education_level         object\n",
    "insured_occupation              object\n",
    "insured_hobbies                 object\n",
    "insured_relationship            object\n",
    "capital-gains                    int64\n",
    "capital-loss                     int64\n",
    "incident_date                   object\n",
    "incident_type                   object\n",
    "collision_type                  object\n",
    "incident_severity               object\n",
    "authorities_contacted           object\n",
    "incident_state                  object\n",
    "incident_city                   object\n",
    "incident_location               object\n",
    "incident_hour_of_the_day         int64\n",
    "number_of_vehicles_involved      int64\n",
    "property_damage                 object\n",
    "bodily_injuries                  int64\n",
    "witnesses                        int64\n",
    "police_report_available         object\n",
    "total_claim_amount               int64\n",
    "injury_claim                     int64\n",
    "property_claim                   int64\n",
    "vehicle_claim                    int64\n",
    "auto_make                       object\n",
    "auto_model                      object\n",
    "auto_year                        int64\n",
    "fraud_reported                  object\n",
    "dtype: object\n",
    "The dataset contains 3 different types of datatypes namely integer data type, float data type and object data type.\n",
    "\n",
    "# Checking number of unique values in each column\n",
    "df.nunique().to_frame(\"No of Unique Values\")\n",
    "No of Unique Values\n",
    "months_as_customer\t391\n",
    "age\t46\n",
    "policy_number\t1000\n",
    "policy_bind_date\t951\n",
    "policy_state\t3\n",
    "policy_csl\t3\n",
    "policy_deductable\t3\n",
    "policy_annual_premium\t991\n",
    "umbrella_limit\t11\n",
    "insured_zip\t995\n",
    "insured_sex\t2\n",
    "insured_education_level\t7\n",
    "insured_occupation\t14\n",
    "insured_hobbies\t20\n",
    "insured_relationship\t6\n",
    "capital-gains\t338\n",
    "capital-loss\t354\n",
    "incident_date\t60\n",
    "incident_type\t4\n",
    "collision_type\t4\n",
    "incident_severity\t4\n",
    "authorities_contacted\t5\n",
    "incident_state\t7\n",
    "incident_city\t7\n",
    "incident_location\t1000\n",
    "incident_hour_of_the_day\t24\n",
    "number_of_vehicles_involved\t4\n",
    "property_damage\t3\n",
    "bodily_injuries\t3\n",
    "witnesses\t4\n",
    "police_report_available\t3\n",
    "total_claim_amount\t763\n",
    "injury_claim\t638\n",
    "property_claim\t626\n",
    "vehicle_claim\t726\n",
    "auto_make\t14\n",
    "auto_model\t39\n",
    "auto_year\t21\n",
    "fraud_reported\t2\n",
    "These are the unique values present in each column.\n",
    "\n",
    "Data Preprocessing\n",
    "Feature selection\n",
    "We can observe the columns policy_number and incident_location have 1000 unique values which means they have only one value count. So it not required for the prediction so we can drop it.\n",
    "\n",
    "# Dropping policy_number and incident_location columns\n",
    "df.drop(\"policy_number\",axis=1,inplace=True)\n",
    "df.drop(\"incident_location\",axis=1,inplace=True)\n",
    "Let's check the list of value counts in each columns to find if there are any unexpected or corrupted entries in the dataset\n",
    "\n",
    "# Checking the value counts of each columns\n",
    "for i in df.columns:\n",
    "        print(df[i].value_counts())\n",
    "        print('*'*100)\n",
    "194    8\n",
    "128    7\n",
    "254    7\n",
    "140    7\n",
    "210    7\n",
    "      ..\n",
    "390    1\n",
    "411    1\n",
    "453    1\n",
    "448    1\n",
    "17     1\n",
    "Name: months_as_customer, Length: 391, dtype: int64\n",
    "****************************************************************************************************\n",
    "43    49\n",
    "39    48\n",
    "41    45\n",
    "34    44\n",
    "38    42\n",
    "30    42\n",
    "31    42\n",
    "37    41\n",
    "33    39\n",
    "40    38\n",
    "32    38\n",
    "29    35\n",
    "46    33\n",
    "42    32\n",
    "35    32\n",
    "36    32\n",
    "44    32\n",
    "28    30\n",
    "26    26\n",
    "45    26\n",
    "48    25\n",
    "47    24\n",
    "27    24\n",
    "57    16\n",
    "25    14\n",
    "55    14\n",
    "49    14\n",
    "53    13\n",
    "50    13\n",
    "24    10\n",
    "54    10\n",
    "61    10\n",
    "51     9\n",
    "60     9\n",
    "58     8\n",
    "56     8\n",
    "23     7\n",
    "21     6\n",
    "59     5\n",
    "62     4\n",
    "52     4\n",
    "64     2\n",
    "63     2\n",
    "19     1\n",
    "20     1\n",
    "22     1\n",
    "Name: age, dtype: int64\n",
    "****************************************************************************************************\n",
    "01-01-2006    3\n",
    "28-04-1992    3\n",
    "05-08-1992    3\n",
    "14-12-1991    2\n",
    "09-08-2004    2\n",
    "             ..\n",
    "03-06-2014    1\n",
    "12-12-1998    1\n",
    "18-02-1999    1\n",
    "30-10-1997    1\n",
    "11-11-1996    1\n",
    "Name: policy_bind_date, Length: 951, dtype: int64\n",
    "****************************************************************************************************\n",
    "OH    352\n",
    "IL    338\n",
    "IN    310\n",
    "Name: policy_state, dtype: int64\n",
    "****************************************************************************************************\n",
    "250/500     351\n",
    "100/300     349\n",
    "500/1000    300\n",
    "Name: policy_csl, dtype: int64\n",
    "****************************************************************************************************\n",
    "1000    351\n",
    "500     342\n",
    "2000    307\n",
    "Name: policy_deductable, dtype: int64\n",
    "****************************************************************************************************\n",
    "1558.29    2\n",
    "1215.36    2\n",
    "1362.87    2\n",
    "1073.83    2\n",
    "1389.13    2\n",
    "          ..\n",
    "1085.03    1\n",
    "1437.33    1\n",
    "988.29     1\n",
    "1238.89    1\n",
    "766.19     1\n",
    "Name: policy_annual_premium, Length: 991, dtype: int64\n",
    "****************************************************************************************************\n",
    " 0           798\n",
    " 6000000      57\n",
    " 5000000      46\n",
    " 4000000      39\n",
    " 7000000      29\n",
    " 3000000      12\n",
    " 8000000       8\n",
    " 9000000       5\n",
    " 2000000       3\n",
    " 10000000      2\n",
    "-1000000       1\n",
    "Name: umbrella_limit, dtype: int64\n",
    "****************************************************************************************************\n",
    "477695    2\n",
    "469429    2\n",
    "446895    2\n",
    "431202    2\n",
    "456602    2\n",
    "         ..\n",
    "476303    1\n",
    "450339    1\n",
    "476502    1\n",
    "600561    1\n",
    "612260    1\n",
    "Name: insured_zip, Length: 995, dtype: int64\n",
    "****************************************************************************************************\n",
    "FEMALE    537\n",
    "MALE      463\n",
    "Name: insured_sex, dtype: int64\n",
    "****************************************************************************************************\n",
    "JD             161\n",
    "High School    160\n",
    "Associate      145\n",
    "MD             144\n",
    "Masters        143\n",
    "PhD            125\n",
    "College        122\n",
    "Name: insured_education_level, dtype: int64\n",
    "****************************************************************************************************\n",
    "machine-op-inspct    93\n",
    "prof-specialty       85\n",
    "tech-support         78\n",
    "sales                76\n",
    "exec-managerial      76\n",
    "craft-repair         74\n",
    "transport-moving     72\n",
    "other-service        71\n",
    "priv-house-serv      71\n",
    "armed-forces         69\n",
    "adm-clerical         65\n",
    "protective-serv      63\n",
    "handlers-cleaners    54\n",
    "farming-fishing      53\n",
    "Name: insured_occupation, dtype: int64\n",
    "****************************************************************************************************\n",
    "reading           64\n",
    "exercise          57\n",
    "paintball         57\n",
    "bungie-jumping    56\n",
    "movies            55\n",
    "golf              55\n",
    "camping           55\n",
    "kayaking          54\n",
    "yachting          53\n",
    "hiking            52\n",
    "video-games       50\n",
    "skydiving         49\n",
    "base-jumping      49\n",
    "board-games       48\n",
    "polo              47\n",
    "chess             46\n",
    "dancing           43\n",
    "sleeping          41\n",
    "cross-fit         35\n",
    "basketball        34\n",
    "Name: insured_hobbies, dtype: int64\n",
    "****************************************************************************************************\n",
    "own-child         183\n",
    "other-relative    177\n",
    "not-in-family     174\n",
    "husband           170\n",
    "wife              155\n",
    "unmarried         141\n",
    "Name: insured_relationship, dtype: int64\n",
    "****************************************************************************************************\n",
    "0        508\n",
    "46300      5\n",
    "51500      4\n",
    "68500      4\n",
    "55600      3\n",
    "        ... \n",
    "36700      1\n",
    "54900      1\n",
    "69200      1\n",
    "48800      1\n",
    "50300      1\n",
    "Name: capital-gains, Length: 338, dtype: int64\n",
    "****************************************************************************************************\n",
    " 0        475\n",
    "-31700      5\n",
    "-53700      5\n",
    "-50300      5\n",
    "-45300      4\n",
    "         ... \n",
    "-12100      1\n",
    "-17000      1\n",
    "-72900      1\n",
    "-19700      1\n",
    "-82100      1\n",
    "Name: capital-loss, Length: 354, dtype: int64\n",
    "****************************************************************************************************\n",
    "02-02-2015    28\n",
    "17-02-2015    26\n",
    "07-01-2015    25\n",
    "10-01-2015    24\n",
    "04-02-2015    24\n",
    "24-01-2015    24\n",
    "19-01-2015    23\n",
    "08-01-2015    22\n",
    "13-01-2015    21\n",
    "30-01-2015    21\n",
    "12-02-2015    20\n",
    "22-02-2015    20\n",
    "31-01-2015    20\n",
    "06-02-2015    20\n",
    "21-02-2015    19\n",
    "01-01-2015    19\n",
    "23-02-2015    19\n",
    "12-01-2015    19\n",
    "14-01-2015    19\n",
    "21-01-2015    19\n",
    "03-01-2015    18\n",
    "14-02-2015    18\n",
    "01-02-2015    18\n",
    "28-02-2015    18\n",
    "20-01-2015    18\n",
    "18-01-2015    18\n",
    "25-02-2015    18\n",
    "06-01-2015    17\n",
    "09-01-2015    17\n",
    "08-02-2015    17\n",
    "24-02-2015    17\n",
    "26-02-2015    17\n",
    "13-02-2015    16\n",
    "15-02-2015    16\n",
    "16-02-2015    16\n",
    "05-02-2015    16\n",
    "16-01-2015    16\n",
    "17-01-2015    15\n",
    "18-02-2015    15\n",
    "28-01-2015    15\n",
    "15-01-2015    15\n",
    "22-01-2015    14\n",
    "20-02-2015    14\n",
    "27-02-2015    14\n",
    "23-01-2015    13\n",
    "03-02-2015    13\n",
    "27-01-2015    13\n",
    "09-02-2015    13\n",
    "04-01-2015    12\n",
    "01-03-2015    12\n",
    "26-01-2015    11\n",
    "29-01-2015    11\n",
    "02-01-2015    11\n",
    "19-02-2015    10\n",
    "11-02-2015    10\n",
    "10-02-2015    10\n",
    "07-02-2015    10\n",
    "25-01-2015    10\n",
    "11-01-2015     9\n",
    "05-01-2015     7\n",
    "Name: incident_date, dtype: int64\n",
    "****************************************************************************************************\n",
    "Multi-vehicle Collision     419\n",
    "Single Vehicle Collision    403\n",
    "Vehicle Theft                94\n",
    "Parked Car                   84\n",
    "Name: incident_type, dtype: int64\n",
    "****************************************************************************************************\n",
    "Rear Collision     292\n",
    "Side Collision     276\n",
    "Front Collision    254\n",
    "?                  178\n",
    "Name: collision_type, dtype: int64\n",
    "****************************************************************************************************\n",
    "Minor Damage      354\n",
    "Total Loss        280\n",
    "Major Damage      276\n",
    "Trivial Damage     90\n",
    "Name: incident_severity, dtype: int64\n",
    "****************************************************************************************************\n",
    "Police       292\n",
    "Fire         223\n",
    "Other        198\n",
    "Ambulance    196\n",
    "None          91\n",
    "Name: authorities_contacted, dtype: int64\n",
    "****************************************************************************************************\n",
    "NY    262\n",
    "SC    248\n",
    "WV    217\n",
    "VA    110\n",
    "NC    110\n",
    "PA     30\n",
    "OH     23\n",
    "Name: incident_state, dtype: int64\n",
    "****************************************************************************************************\n",
    "Springfield    157\n",
    "Arlington      152\n",
    "Columbus       149\n",
    "Northbend      145\n",
    "Hillsdale      141\n",
    "Riverwood      134\n",
    "Northbrook     122\n",
    "Name: incident_city, dtype: int64\n",
    "****************************************************************************************************\n",
    "17    54\n",
    "3     53\n",
    "0     52\n",
    "23    51\n",
    "16    49\n",
    "13    46\n",
    "10    46\n",
    "4     46\n",
    "6     44\n",
    "9     43\n",
    "14    43\n",
    "21    42\n",
    "18    41\n",
    "12    40\n",
    "19    40\n",
    "7     40\n",
    "15    39\n",
    "22    38\n",
    "8     36\n",
    "20    34\n",
    "5     33\n",
    "2     31\n",
    "11    30\n",
    "1     29\n",
    "Name: incident_hour_of_the_day, dtype: int64\n",
    "****************************************************************************************************\n",
    "1    581\n",
    "3    358\n",
    "4     31\n",
    "2     30\n",
    "Name: number_of_vehicles_involved, dtype: int64\n",
    "****************************************************************************************************\n",
    "?      360\n",
    "NO     338\n",
    "YES    302\n",
    "Name: property_damage, dtype: int64\n",
    "****************************************************************************************************\n",
    "0    340\n",
    "2    332\n",
    "1    328\n",
    "Name: bodily_injuries, dtype: int64\n",
    "****************************************************************************************************\n",
    "1    258\n",
    "2    250\n",
    "0    249\n",
    "3    243\n",
    "Name: witnesses, dtype: int64\n",
    "****************************************************************************************************\n",
    "?      343\n",
    "NO     343\n",
    "YES    314\n",
    "Name: police_report_available, dtype: int64\n",
    "****************************************************************************************************\n",
    "59400    5\n",
    "2640     4\n",
    "70400    4\n",
    "4320     4\n",
    "44200    4\n",
    "        ..\n",
    "65250    1\n",
    "87100    1\n",
    "6240     1\n",
    "66600    1\n",
    "67500    1\n",
    "Name: total_claim_amount, Length: 763, dtype: int64\n",
    "****************************************************************************************************\n",
    "0        25\n",
    "640       7\n",
    "480       7\n",
    "660       5\n",
    "580       5\n",
    "         ..\n",
    "14840     1\n",
    "6580      1\n",
    "11820     1\n",
    "16650     1\n",
    "7500      1\n",
    "Name: injury_claim, Length: 638, dtype: int64\n",
    "****************************************************************************************************\n",
    "0        19\n",
    "860       6\n",
    "480       5\n",
    "660       5\n",
    "10000     5\n",
    "         ..\n",
    "3590      1\n",
    "6480      1\n",
    "4580      1\n",
    "4920      1\n",
    "7500      1\n",
    "Name: property_claim, Length: 626, dtype: int64\n",
    "****************************************************************************************************\n",
    "5040     7\n",
    "3360     6\n",
    "52080    5\n",
    "4720     5\n",
    "3600     5\n",
    "        ..\n",
    "43360    1\n",
    "25130    1\n",
    "38940    1\n",
    "47430    1\n",
    "52500    1\n",
    "Name: vehicle_claim, Length: 726, dtype: int64\n",
    "****************************************************************************************************\n",
    "Saab          80\n",
    "Dodge         80\n",
    "Suburu        80\n",
    "Nissan        78\n",
    "Chevrolet     76\n",
    "Ford          72\n",
    "BMW           72\n",
    "Toyota        70\n",
    "Audi          69\n",
    "Accura        68\n",
    "Volkswagen    68\n",
    "Jeep          67\n",
    "Mercedes      65\n",
    "Honda         55\n",
    "Name: auto_make, dtype: int64\n",
    "****************************************************************************************************\n",
    "RAM               43\n",
    "Wrangler          42\n",
    "A3                37\n",
    "Neon              37\n",
    "MDX               36\n",
    "Jetta             35\n",
    "Passat            33\n",
    "A5                32\n",
    "Legacy            32\n",
    "Pathfinder        31\n",
    "Malibu            30\n",
    "92x               28\n",
    "Camry             28\n",
    "Forrestor         28\n",
    "F150              27\n",
    "95                27\n",
    "E400              27\n",
    "93                25\n",
    "Grand Cherokee    25\n",
    "Escape            24\n",
    "Tahoe             24\n",
    "Maxima            24\n",
    "Ultima            23\n",
    "X5                23\n",
    "Highlander        22\n",
    "Civic             22\n",
    "Silverado         22\n",
    "Fusion            21\n",
    "ML350             20\n",
    "Impreza           20\n",
    "Corolla           20\n",
    "TL                20\n",
    "CRV               20\n",
    "C300              18\n",
    "3 Series          18\n",
    "X6                16\n",
    "M5                15\n",
    "Accord            13\n",
    "RSX               12\n",
    "Name: auto_model, dtype: int64\n",
    "****************************************************************************************************\n",
    "1995    56\n",
    "1999    55\n",
    "2005    54\n",
    "2006    53\n",
    "2011    53\n",
    "2007    52\n",
    "2003    51\n",
    "2009    50\n",
    "2010    50\n",
    "2013    49\n",
    "2002    49\n",
    "2015    47\n",
    "1997    46\n",
    "2012    46\n",
    "2008    45\n",
    "2014    44\n",
    "2001    42\n",
    "2000    42\n",
    "1998    40\n",
    "2004    39\n",
    "1996    37\n",
    "Name: auto_year, dtype: int64\n",
    "****************************************************************************************************\n",
    "N    753\n",
    "Y    247\n",
    "Name: fraud_reported, dtype: int64\n",
    "****************************************************************************************************\n",
    "These are the list of value counts in each columns.\n",
    "\n",
    "By looking at the value counts of each column we can realize that the columns umbrella_limit, capital-gains and capital-loss contains more zero values around 79.8%, 50.8% and 47.5%. I am keeping the zero values in capital_gains and capital_loss columns as it is. Since the umbrella_limit columns has more that 70% of zero values, let's drop that column.\n",
    "\n",
    "# Dropping umbrella_limit column\n",
    "df.drop(\"umbrella_limit\",axis=1,inplace=True)\n",
    "The column insured_zip is the zip code given to each person. If we take a look at the value count and unique values of the column insured_zip, it contains 995 unique values that means the 5 entries are repeating. Since it is giving some information about the person, either we can drop this or we can convert its data type from integer to object for better processing.\n",
    "\n",
    "# Dropping insured_zip column as it is not important for the prediction\n",
    "df.drop('insured_zip',axis=1,inplace=True)\n",
    "Feature Extraction\n",
    "The policy_bind_date and incident_date have object data type which should be in datetime data type that means the python is not able to understand the type of these column and giving default data type. We will convert this object data type to datetime data type and we will extract the vaues from these columns.\n",
    "\n",
    "# Converting Date columns from object type into datetime data type\n",
    "df['policy_bind_date']=pd.to_datetime(df['policy_bind_date'])\n",
    "df['incident_date']=pd.to_datetime(df['incident_date'])\n",
    "# Again checking the type of dataset\n",
    "df.dtypes\n",
    "months_as_customer                      int64\n",
    "age                                     int64\n",
    "policy_bind_date               datetime64[ns]\n",
    "policy_state                           object\n",
    "policy_csl                             object\n",
    "policy_deductable                       int64\n",
    "policy_annual_premium                 float64\n",
    "insured_sex                            object\n",
    "insured_education_level                object\n",
    "insured_occupation                     object\n",
    "insured_hobbies                        object\n",
    "insured_relationship                   object\n",
    "capital-gains                           int64\n",
    "capital-loss                            int64\n",
    "incident_date                  datetime64[ns]\n",
    "incident_type                          object\n",
    "collision_type                         object\n",
    "incident_severity                      object\n",
    "authorities_contacted                  object\n",
    "incident_state                         object\n",
    "incident_city                          object\n",
    "incident_hour_of_the_day                int64\n",
    "number_of_vehicles_involved             int64\n",
    "property_damage                        object\n",
    "bodily_injuries                         int64\n",
    "witnesses                               int64\n",
    "police_report_available                object\n",
    "total_claim_amount                      int64\n",
    "injury_claim                            int64\n",
    "property_claim                          int64\n",
    "vehicle_claim                           int64\n",
    "auto_make                              object\n",
    "auto_model                             object\n",
    "auto_year                               int64\n",
    "fraud_reported                         object\n",
    "dtype: object\n",
    "Now we have converted object data type into datetime data type. Now let's extract Day, Month and Year from both the columns.\n",
    "\n",
    "# Extracting Day, Month and Year column from policy_bind_date\n",
    "df[\"policy_bind_Day\"] = df['policy_bind_date'].dt.day\n",
    "df[\"policy_bind_Month\"] = df['policy_bind_date'].dt.month\n",
    "df[\"policy_bind_Year\"] = df['policy_bind_date'].dt.year\n",
    "\n",
    "# Extracting Day, Month and Year column from incident_date\n",
    "df[\"incident_Day\"] = df['incident_date'].dt.day\n",
    "df[\"incident_Month\"] = df['incident_date'].dt.month\n",
    "df[\"incident_Year\"] = df['incident_date'].dt.year\n",
    "Now we have extracted Day, Month and Year columns from both policy_bind_date and incident_date columns. So we can drop these columns.\n",
    "\n",
    "# Dropping policy_bind_date and incident_date columns\n",
    "df.drop([\"policy_bind_date\",\"incident_date\"],axis=1,inplace=True)\n",
    "We have dropped the columns policy_bind_date and incident_date as we have extracted the required data from those columns.\n",
    "\n",
    "From the value counts of the columns we can also observe the some columns have \"?\" values, they are not NAN values but we need to fill them.\n",
    "\n",
    "# Checking which columns contains \"?\" sign\n",
    "df[df.columns[(df == '?').any()]].nunique()\n",
    "collision_type             4\n",
    "property_damage            3\n",
    "police_report_available    3\n",
    "dtype: int64\n",
    "These are the columns which contains \"?\" sign. Since these column seems to be categorical so we will replace \"?\" values with most frequently occuring values of the respective columns that is their mode values.\n",
    "\n",
    "# Checking mode of the above columns\n",
    "print(\"The mode of collision_type is:\",df[\"collision_type\"].mode())\n",
    "print(\"The mode of property_damage is:\",df[\"property_damage\"].mode())\n",
    "print(\"The mode of police_report_available is:\",df[\"police_report_available\"].mode())\n",
    "The mode of collision_type is: 0    Rear Collision\n",
    "Name: collision_type, dtype: object\n",
    "The mode of property_damage is: 0    ?\n",
    "Name: property_damage, dtype: object\n",
    "The mode of police_report_available is: 0     ?\n",
    "1    NO\n",
    "Name: police_report_available, dtype: object\n",
    "The mode of property_damage and police_report_available is \"?\", which means the data is almost covered by \"?\" sign. So we will fill them by the second highest count of the respective column.\n",
    "\n",
    "# Checking value count of property_damage column and police_report_available\n",
    "print(\"The value count of property_damage:\\n\", df[\"property_damage\"].value_counts())\n",
    "print(\"\\n\")\n",
    "print(\"The value count of police_report_available:\\n\", df[\"police_report_available\"].value_counts())\n",
    "The value count of property_damage:\n",
    " ?      360\n",
    "NO     338\n",
    "YES    302\n",
    "Name: property_damage, dtype: int64\n",
    "\n",
    "\n",
    "The value count of police_report_available:\n",
    " ?      343\n",
    "NO     343\n",
    "YES    314\n",
    "Name: police_report_available, dtype: int64\n",
    "Here \"NO\" is the second highest occured category in both property_damage and police_report_available column. We will replace \"?\" by \"NO\".\n",
    "\n",
    "# Replacing \"?\" by their mode values\n",
    "df['collision_type'] = df.collision_type.str.replace('?', df['collision_type'].mode()[0])\n",
    "df['property_damage'] = df.property_damage.str.replace('?', \"NO\")\n",
    "df['police_report_available'] = df.police_report_available.str.replace('?', \"NO\")\n",
    "# Checking value count again\n",
    "print(\"The value count of collision_type:\\n\", df[\"collision_type\"].value_counts())\n",
    "print(\"\\n\")\n",
    "print(\"The value count of property_damage:\\n\", df[\"property_damage\"].value_counts())\n",
    "print(\"\\n\")\n",
    "print(\"The value count of police_report_available:\\n\", df[\"police_report_available\"].value_counts())\n",
    "The value count of collision_type:\n",
    " Rear Collision     470\n",
    "Side Collision     276\n",
    "Front Collision    254\n",
    "Name: collision_type, dtype: int64\n",
    "\n",
    "\n",
    "The value count of property_damage:\n",
    " NO     698\n",
    "YES    302\n",
    "Name: property_damage, dtype: int64\n",
    "\n",
    "\n",
    "The value count of police_report_available:\n",
    " NO     686\n",
    "YES    314\n",
    "Name: police_report_available, dtype: int64\n",
    "Here we have replaced the \"?\" sign by mode and \"NO\" values.\n",
    "\n",
    "The policy_csl column showing object data type but it contains numerical data, may be it is because of the presence of \"/\" in that column. So first we will extract two columns csl_per_person and csl_per_accident from policy_csl colums and then will convert their object data type into integer data type.\n",
    "\n",
    "# Extracting csl_per_person and csl_per_accident from policy_csl column\n",
    "df['csl_per_person'] = df.policy_csl.str.split('/', expand=True)[0]\n",
    "df['csl_per_accident'] = df.policy_csl.str.split('/', expand=True)[1]\n",
    "# Converting object data type into integer data type\n",
    "df['csl_per_person'] = df['csl_per_person'].astype('int64')\n",
    "df['csl_per_accident'] = df['csl_per_accident'].astype('int64')\n",
    "# Since we have extracted the data from policy_csl, let's drop that column\n",
    "df.drop(\"policy_csl\",axis=1,inplace=True)\n",
    "# Let's extract age of the vehicle from auto_year by subtracting it from the year 2018\n",
    "df[\"Vehicle_Age\"] = 2018 - df[\"auto_year\"]\n",
    "df.drop(\"auto_year\",axis=1, inplace = True)\n",
    "Here we have extracted age of the vehicle on the basis of auto year by assuming the data is collected in the year 2018.\n",
    "\n",
    "# Let's check the unique values again\n",
    "df.nunique().to_frame(\"No of Unique Values\")\n",
    "No of Unique Values\n",
    "months_as_customer\t391\n",
    "age\t46\n",
    "policy_state\t3\n",
    "policy_deductable\t3\n",
    "policy_annual_premium\t991\n",
    "insured_sex\t2\n",
    "insured_education_level\t7\n",
    "insured_occupation\t14\n",
    "insured_hobbies\t20\n",
    "insured_relationship\t6\n",
    "capital-gains\t338\n",
    "capital-loss\t354\n",
    "incident_type\t4\n",
    "collision_type\t3\n",
    "incident_severity\t4\n",
    "authorities_contacted\t5\n",
    "incident_state\t7\n",
    "incident_city\t7\n",
    "incident_hour_of_the_day\t24\n",
    "number_of_vehicles_involved\t4\n",
    "property_damage\t2\n",
    "bodily_injuries\t3\n",
    "witnesses\t4\n",
    "police_report_available\t2\n",
    "total_claim_amount\t763\n",
    "injury_claim\t638\n",
    "property_claim\t626\n",
    "vehicle_claim\t726\n",
    "auto_make\t14\n",
    "auto_model\t39\n",
    "fraud_reported\t2\n",
    "policy_bind_Day\t31\n",
    "policy_bind_Month\t12\n",
    "policy_bind_Year\t26\n",
    "incident_Day\t22\n",
    "incident_Month\t12\n",
    "incident_Year\t1\n",
    "csl_per_person\t3\n",
    "csl_per_accident\t3\n",
    "Vehicle_Age\t21\n",
    "These are the unique values present in each column after feature extraction and selection. Here incident_Year column has one unique value through out the column also it is not important for our prediction so we can drop this column.\n",
    "\n",
    "# Dropping incident_Year column \n",
    "df.drop(\"incident_Year\", axis=1, inplace=True)\n",
    "# Let's take a look at the dataframe after preprocessing\n",
    "df.head()\n",
    "months_as_customer\tage\tpolicy_state\tpolicy_deductable\tpolicy_annual_premium\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tcapital-gains\tcapital-loss\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tproperty_damage\tbodily_injuries\twitnesses\tpolice_report_available\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tauto_make\tauto_model\tfraud_reported\tpolicy_bind_Day\tpolicy_bind_Month\tpolicy_bind_Year\tincident_Day\tincident_Month\tcsl_per_person\tcsl_per_accident\tVehicle_Age\n",
    "0\t328\t48\tOH\t1000\t1406.91\tMALE\tMD\tcraft-repair\tsleeping\thusband\t53300\t0\tSingle Vehicle Collision\tSide Collision\tMajor Damage\tPolice\tSC\tColumbus\t5\t1\tYES\t1\t2\tYES\t71610\t6510\t13020\t52080\tSaab\t92x\tY\t17\t10\t2014\t25\t1\t250\t500\t14\n",
    "1\t228\t42\tIN\t2000\t1197.22\tMALE\tMD\tmachine-op-inspct\treading\tother-relative\t0\t0\tVehicle Theft\tRear Collision\tMinor Damage\tPolice\tVA\tRiverwood\t8\t1\tNO\t0\t0\tNO\t5070\t780\t780\t3510\tMercedes\tE400\tY\t27\t6\t2006\t21\t1\t250\t500\t11\n",
    "2\t134\t29\tOH\t2000\t1413.14\tFEMALE\tPhD\tsales\tboard-games\town-child\t35100\t0\tMulti-vehicle Collision\tRear Collision\tMinor Damage\tPolice\tNY\tColumbus\t7\t3\tNO\t2\t3\tNO\t34650\t7700\t3850\t23100\tDodge\tRAM\tN\t9\t6\t2000\t22\t2\t100\t300\t11\n",
    "3\t256\t41\tIL\t2000\t1415.74\tFEMALE\tPhD\tarmed-forces\tboard-games\tunmarried\t48900\t-62400\tSingle Vehicle Collision\tFront Collision\tMajor Damage\tPolice\tOH\tArlington\t5\t1\tNO\t1\t2\tNO\t63400\t6340\t6340\t50720\tChevrolet\tTahoe\tY\t25\t5\t1990\t1\t10\t250\t500\t4\n",
    "4\t228\t44\tIL\t1000\t1583.91\tMALE\tAssociate\tsales\tboard-games\tunmarried\t66000\t-46000\tVehicle Theft\tRear Collision\tMinor Damage\tNone\tNY\tArlington\t20\t1\tNO\t0\t1\tNO\t6500\t1300\t650\t4550\tAccura\tRSX\tN\t6\t6\t2014\t17\t2\t500\t1000\t9\n",
    "df.shape\n",
    "(1000, 39)\n",
    "# Separating numerical and categorcal columns\n",
    "\n",
    "# Checking for categorical columns\n",
    "categorical_col=[]\n",
    "for i in df.dtypes.index:\n",
    "    if df.dtypes[i]=='object':\n",
    "        categorical_col.append(i)\n",
    "print(\"Categorical columns are:\\n\",categorical_col)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Now checking for numerical columns\n",
    "numerical_col=[]\n",
    "for i in df.dtypes.index:\n",
    "    if df.dtypes[i]!='object':\n",
    "        numerical_col.append(i)\n",
    "print(\"Numerical columns are:\\n\",numerical_col)\n",
    "Categorical columns are:\n",
    " ['policy_state', 'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'property_damage', 'police_report_available', 'auto_make', 'auto_model', 'fraud_reported']\n",
    "\n",
    "\n",
    "Numerical columns are:\n",
    " ['months_as_customer', 'age', 'policy_deductable', 'policy_annual_premium', 'capital-gains', 'capital-loss', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'policy_bind_Day', 'policy_bind_Month', 'policy_bind_Year', 'incident_Day', 'incident_Month', 'csl_per_person', 'csl_per_accident', 'Vehicle_Age']\n",
    "These are the categorical and numerical columns present in the dataset.\n",
    "\n",
    "# Checking the list of counts of target\n",
    "df['fraud_reported'].unique()\n",
    "array(['Y', 'N'], dtype=object)\n",
    "There are two unique categories in fraud_reported column these values tells about if an insurance claim is fraudulent or not. Here we can assume that \"Y\" stands for \"Yes\" that is the insurance is fraudulent and \"N\" stands for \"No\" means the insurance claim is not fraudulent.\n",
    "\n",
    "# Checking the unique values in target column\n",
    "df['fraud_reported'].value_counts()\n",
    "N    753\n",
    "Y    247\n",
    "Name: fraud_reported, dtype: int64\n",
    "From the value count of the target column fraud_reported we can notice the count of the categories are different which means the data is not balanced. We will use oversampling method to balance the data before building the models.\n",
    "\n",
    "Statistical summary of numerical columns\n",
    "df.describe()\n",
    "months_as_customer\tage\tpolicy_deductable\tpolicy_annual_premium\tcapital-gains\tcapital-loss\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tbodily_injuries\twitnesses\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tpolicy_bind_Day\tpolicy_bind_Month\tpolicy_bind_Year\tincident_Day\tincident_Month\tcsl_per_person\tcsl_per_accident\tVehicle_Age\n",
    "count\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.00000\t1000.000000\t1000.000000\t1000.00000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\t1000.00000\t1000.000000\t1000.000000\t1000.000000\t1000.000000\n",
    "mean\t203.954000\t38.948000\t1136.000000\t1256.406150\t25126.100000\t-26793.700000\t11.644000\t1.83900\t0.992000\t1.487000\t52761.94000\t7433.420000\t7399.570000\t37928.950000\t15.448000\t6.559000\t2001.604000\t13.08400\t3.407000\t272.650000\t580.200000\t12.897000\n",
    "std\t115.113174\t9.140287\t611.864673\t244.167395\t27872.187708\t28104.096686\t6.951373\t1.01888\t0.820127\t1.111335\t26401.53319\t4880.951853\t4824.726179\t18886.252893\t8.850176\t3.391758\t7.360391\t10.44318\t3.276291\t161.603196\t287.420547\t6.015861\n",
    "min\t0.000000\t19.000000\t500.000000\t433.330000\t0.000000\t-111100.000000\t0.000000\t1.00000\t0.000000\t0.000000\t100.00000\t0.000000\t0.000000\t70.000000\t1.000000\t1.000000\t1990.000000\t1.00000\t1.000000\t100.000000\t300.000000\t3.000000\n",
    "25%\t115.750000\t32.000000\t500.000000\t1089.607500\t0.000000\t-51500.000000\t6.000000\t1.00000\t0.000000\t1.000000\t41812.50000\t4295.000000\t4445.000000\t30292.500000\t8.000000\t4.000000\t1995.000000\t2.00000\t1.000000\t100.000000\t300.000000\t8.000000\n",
    "50%\t199.500000\t38.000000\t1000.000000\t1257.200000\t0.000000\t-23250.000000\t12.000000\t1.00000\t1.000000\t1.000000\t58055.00000\t6775.000000\t6750.000000\t42100.000000\t16.000000\t7.000000\t2002.000000\t15.00000\t2.000000\t250.000000\t500.000000\t13.000000\n",
    "75%\t276.250000\t44.000000\t2000.000000\t1415.695000\t51025.000000\t0.000000\t17.000000\t3.00000\t2.000000\t2.000000\t70592.50000\t11305.000000\t10885.000000\t50822.500000\t23.000000\t9.000000\t2008.000000\t22.00000\t5.000000\t500.000000\t1000.000000\t18.000000\n",
    "max\t479.000000\t64.000000\t2000.000000\t2047.590000\t100500.000000\t0.000000\t23.000000\t4.00000\t2.000000\t3.000000\t114920.00000\t21450.000000\t23670.000000\t79560.000000\t31.000000\t12.000000\t2015.000000\t31.00000\t12.000000\t500.000000\t1000.000000\t23.000000\n",
    "This gives the statistical information of the numerical columns present in the dataframe. The summary of this dataset looks perfect since there is no negative/ invalid values present.\n",
    "\n",
    "From the above description we can observe the following things:\n",
    "\n",
    "Here the counts of all the columns are equal which means there are no missing values in the dataset. In some of the columns like policy_deductable, capital-gains, injury_claim etc we can observe the mean value is greater than the median(50%) which means the data in those columns are skewed to right. And in some of the columns like total_claim_amount, vehicle_claim...etc we can observe the median is greater than the mean which means the data in the columns are skewed to left. And some of the columns have equal mean and median that means the data symmetric and is normally distributed and no skewness preaent. There is a huge difference in 75% and max it shows that huge outliers present in the columns.\n",
    "\n",
    "Data Visualization\n",
    "Univariate Analysis\n",
    "Plotting categorical columns\n",
    "#Visualizing how many insurance claims is fraudulent\n",
    "print(df[\"fraud_reported\"].value_counts())\n",
    "sns.countplot(df[\"fraud_reported\"],palette=\"Dark2\")\n",
    "plt.show()\n",
    "N    753\n",
    "Y    247\n",
    "Name: fraud_reported, dtype: int64\n",
    "\n",
    "From the plot we can observe that the count of \"N\" is high compared to \"N\". Which means Here we can assume that \"Y\" stands for \"Yes\" that is the insurance is fraudulent and \"N\" stands for \"No\" means the insurance claim is not fraudulent. Here most of the insurance claims have not reported as fradulent.\n",
    "\n",
    "Since it is our target column, it indicates the class imbalance issue. We will balance the data using oversampling method in later part.\n",
    "\n",
    "Visualize data using Pie chart\n",
    "def generate_pie(i):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.pie(i.value_counts(), labels=i.value_counts().index, autopct='%1.2f%%',shadow=True,)\n",
    "    plt.legend(prop={'size':14})\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    return plt.show()\n",
    "\n",
    "cols1 = ['policy_state', 'insured_sex', 'insured_education_level','insured_relationship', 'incident_type','collision_type','property_damage','police_report_available']\n",
    "\n",
    "plotnumber=1\n",
    "for j in df[cols1]:\n",
    "    print(f\"Pie plot for the column:\", j)\n",
    "    generate_pie(df[j])\n",
    "Pie plot for the column: policy_state\n",
    "\n",
    "Pie plot for the column: insured_sex\n",
    "\n",
    "Pie plot for the column: insured_education_level\n",
    "\n",
    "Pie plot for the column: insured_relationship\n",
    "\n",
    "Pie plot for the column: incident_type\n",
    "\n",
    "Pie plot for the column: collision_type\n",
    "\n",
    "Pie plot for the column: property_damage\n",
    "\n",
    "Pie plot for the column: police_report_available\n",
    "\n",
    "Above are the pie plots for some of the categorical columns. From the above pie plots we can observe the following things:\n",
    "\n",
    "The types of the policies claimed by the customers are almost same but still the policy state type HO has bit high counts and the type IL has bit less count.\n",
    "Both male and female have insurance but the count for Female is bit higher than Male counts.\n",
    "The count is pretty much same for all the education level but still the people who have completed their college and PhD have less count compared to others.\n",
    "Similar to insured education, insured relationship is also almost equally distributed.\n",
    "While looking at the incident type, Multi-vehicle collision and Single Vehicle Collision have pretty much similar counts of around 41% and 400%. But the count is very less in Parked car and Vehicle Theft.\n",
    "The collision type has 3 different types . In this the count is high in Rear collision and the other two types have almost equal counts.\n",
    "As we observe the propert damage plot, around 69% of the people did not face any property damage while 30% of the people faced the property damage.\n",
    "Over 68% of the people were produced the police reports while 31% of the people didn't submit any police reports.\n",
    "Count plots\n",
    "cols2 = ['insured_occupation', 'insured_hobbies', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city','auto_make', 'auto_model']\n",
    "\n",
    "plt.figure(figsize=(15,25),facecolor='white')\n",
    "plotnumber=1\n",
    "for column in cols2:\n",
    "    if plotnumber:\n",
    "        ax=plt.subplot(5,2,plotnumber)\n",
    "        sns.countplot(df[column],palette=\"gnuplot\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.xlabel(column,fontsize=15)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "Above are the count plots for the remaining categorical columns. From the count plots we can observe the following things:\n",
    "\n",
    "In the insured occupation we can observe most of the data is covered by machine operation inspector followed by professional speciality. Apart from this all the other insured occupations have almost same couns.\n",
    "With respect to insured hobbies, we can notice reading covered the highest data followed by exercise. And other categories have the average counts.\n",
    "The incodent severity count is high for Minor damages and trivial damage data has very less count compared to others.\n",
    "When the accidents occurs then most of the authorities contacts the police, here the the category police covers highest data and Fire having the second highest count. But Ambulance and Others have almost same counts and the count is very less for None compared to all.\n",
    "With respect to the incident state, New York, South Carolina and West Virginia states have highest counts.\n",
    "In incident city, almost all the columns have equal counts.\n",
    "When we look at the vehicle manufactured companies, the categories Saab, Suburu, Dodge, Nissan and Volkswagen have highest counts.\n",
    "When we take a look at the vehicle models then RAM and Wrangler automobile models have highest counts and also RSX and Accord have very less count.\n",
    "Distribution of skewness\n",
    "a) Plotting numerical columns\n",
    "\n",
    "# Checking how the data has been distriubted in each column\n",
    "\n",
    "plt.figure(figsize=(25,35),facecolor='white')\n",
    "plotnumber=1\n",
    "for column in numerical_col:\n",
    "    if plotnumber<=23:\n",
    "        ax=plt.subplot(6,4,plotnumber)\n",
    "        sns.distplot(df[column],color=\"darkgreen\",hist=False,kde_kws={\"shade\": True})\n",
    "        plt.xlabel(column,fontsize=18)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "\n",
    "Above are the distribution plots for all the numerical columns. From the distplots we can observe the following things:\n",
    "\n",
    "The data is normally distributed in most of the columns.\n",
    "Some of the columns like capital gains and incident months have mean value greater than the median, hence they are skewed to right.\n",
    "The data in the column capital loss is skewed to left since the median is greater than the mean.\n",
    "We will remove the skewness using appropriate methods in the later part.\n",
    "Bivariate Analysis\n",
    "# Comparision between two variables\n",
    "plt.figure(figsize=[18,13])\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Comparision between months_as_customer and age')\n",
    "sns.scatterplot(df['months_as_customer'],df['age'],hue=df['fraud_reported']);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Comparision between total_claim_amount and injury_claim')\n",
    "sns.scatterplot(df['total_claim_amount'],df['injury_claim'],hue=df['fraud_reported']);\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Comparision between property_claim and vehicle_claim')\n",
    "sns.scatterplot(df['property_claim'],df['vehicle_claim'],hue=df['fraud_reported']);\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Comparision between months_as_customer and total_claim_amount')\n",
    "sns.scatterplot(df['months_as_customer'],df['total_claim_amount'],hue=df['fraud_reported']);\n",
    "\n",
    "From the above scatter plot we can observe the following things:\n",
    "\n",
    "There is a positive linear relation between age and month_as_customer column. As age increases the month_as customers also increases, also the fraud reported is very less in htis case.\n",
    "In the second graph we cna observe the positive linear relation, as total cliam amount increases, injury claim is also increases.\n",
    "Third plot is also same as second one that is as the property claim increases, vehicle claim is also increases.\n",
    "In the fourth plot we can observe the data is scattered and there is no much relation between the features.\n",
    "fig,axes=plt.subplots(2,2,figsize=(12,10))\n",
    "\n",
    "# Comparing insured_sex and age\n",
    "sns.violinplot(x='insured_sex',y='age',ax=axes[0,0],data=df,palette=\"ch:.25\",hue=\"fraud_reported\",split=True)\n",
    "\n",
    "# Comparing policy_state and witnesses\n",
    "sns.violinplot(x='policy_state',y='witnesses',ax=axes[0,1],data=df,hue=\"fraud_reported\",split=True,palette=\"hls\")\n",
    "\n",
    "# Comparing csl_per_accident and property_claim\n",
    "sns.violinplot(x='csl_per_accident',y='property_claim',ax=axes[1,0],data=df,hue=\"fraud_reported\",split=True,palette=\"Dark2\")\n",
    "\n",
    "# Comparing csl_per_person and age\n",
    "sns.violinplot(x='csl_per_person',y='age',ax=axes[1,1],data=df,hue=\"fraud_reported\",split=True,palette=\"mako\")\n",
    "plt.show()\n",
    "\n",
    "The fraud report is high for both the males females having age between 30-45.\n",
    "The people who own the policy state \"IN\" have high fraud report.\n",
    "The person who has csl per accident insurance by claiming property in the range 5000-15000 have the fraud report.\n",
    "The csl_per_person with age 30-45 are facing the fraudulent reports.\n",
    "fig,axes=plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "# Comparing insured_sex and age\n",
    "sns.violinplot(x='fraud_reported',y='total_claim_amount',ax=axes[0,0],data=df,color=\"g\")\n",
    "\n",
    "# Comparing policy_state and witnesses\n",
    "sns.violinplot(x='fraud_reported',y='vehicle_claim',ax=axes[0,1],data=df,hue=\"fraud_reported\",palette=\"cool_r\")\n",
    "\n",
    "# Comparing csl_per_accident and property_claim\n",
    "sns.violinplot(x='fraud_reported',y='property_claim',ax=axes[1,0],data=df,hue=\"fraud_reported\",palette=\"cividis\")\n",
    "\n",
    "# Comparing csl_per_person and age\n",
    "sns.violinplot(x='fraud_reported',y='injury_claim',ax=axes[1,1],data=df,hue=\"fraud_reported\",palette=\"gnuplot2\")\n",
    "plt.show()\n",
    "\n",
    "Most of the fraud reports found when the total claimed amount is 5000-70000.\n",
    "The fraud report is high when the claimed vehicle is between 3700-5900.\n",
    "The frad reports is high when the property claimed is between 5200-8500.\n",
    "Most fraud reported when injury claim are between 5000 to 8000.\n",
    "# Comparing policy_state and fraud_reported\n",
    "sns.factorplot('policy_state',kind='count',data=df,hue='fraud_reported',palette=\"Dark2\")\n",
    "plt.show()\n",
    "\n",
    "Fraud report is bit high in the \"OH\" policy state.\n",
    "\n",
    "# Comparing insured_education_level and fraud_reported\n",
    "\n",
    "sns.factorplot('insured_education_level',kind='count',data=df,hue='fraud_reported',palette=\"tab20b_r\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The fraudulent level is very less for the people who have high school education and the people who have completed their \"JD\" education have high fraud report. The people who have high insured education are facing insurance fraudulent compared to the people with less insured education level.\n",
    "\n",
    "# Comparing insured_occupation and fraud_reported\n",
    "sns.factorplot('insured_occupation',kind='count',data=df,hue='fraud_reported',palette=\"spring_r\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The people who are in the position exec-managerials have high fraud reports compared to others.\n",
    "\n",
    "# Comparing insured_hobbies and fraud_reported\n",
    "sns.factorplot('insured_hobbies',kind='count',data=df,hue='fraud_reported',palette=\"gnuplot2\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The fraud report is high for the people who have the hobby of playing chess and cross fit.\n",
    "\n",
    "# Comparing insured_relationship and fraud_reported\n",
    "sns.factorplot('insured_relationship',kind='count',data=df,hue='fraud_reported',palette=\"gist_earth\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The fraud report is high for the customers who have other relative and it is very less for unmarried people.\n",
    "\n",
    "# Comparing incident_type and fraud_reported\n",
    "sns.factorplot('incident_type',kind='count',data=df,hue='fraud_reported',palette=\"Set2_r\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "In Multivehicle collission and single vehicle collision, the fraud report is very high compared to others.\n",
    "\n",
    "# Comparing collision_type and fraud_reported\n",
    "sns.factorplot('collision_type',kind='count',data=df,hue='fraud_reported',palette=\"gist_earth\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The fraud level is high in the collision type Rear Collision and other two collision type have average reports.\n",
    "\n",
    "# Comparing incident_severity and fraud_reported\n",
    "sns.factorplot('incident_severity',kind='count',data=df,hue='fraud_reported',palette=\"mako\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The fraud report is high in Major damage incident severity and for Trivial Damage the report is less compared to others.\n",
    "\n",
    "# Comparing authorities_contacted and fraud_reported\n",
    "sns.factorplot('authorities_contacted',kind='count',data=df,hue='fraud_reported',palette=\"magma\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The police contacted cases are very high and the fraud report is in equal for all the authorities except None.\n",
    "\n",
    "# Comparing incident_state and fraud_reported\n",
    "sns.factorplot('incident_state',kind='count',data=df,col='fraud_reported',palette=\"cubehelix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The state SC has high fraud reports compared to other states.\n",
    "\n",
    "# Comparing incident_city and fraud_reported\n",
    "sns.catplot('incident_city',kind='count',data=df,hue='fraud_reported',palette=\"bright\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "The cities Riverwood and Northbrook have very less fraud reports compared to others.\n",
    "\n",
    "# Comparing property_damage and fraud_reported\n",
    "sns.factorplot('property_damage',kind='count',data=df,col='fraud_reported',palette=\"ocean\")\n",
    "plt.show()\n",
    "\n",
    "The customers who do not have any property damage case they have high fraud reports.\n",
    "\n",
    "# Comparing police_report_available and fraud_reported\n",
    "sns.factorplot('police_report_available',kind='count',data=df,col='fraud_reported',palette=\"coolwarm\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "If there is no police report available then the fraud report is very high.\n",
    "\n",
    "# Comparing auto_make and fraud_reported\n",
    "sns.catplot('auto_make',kind='count',data=df,hue='fraud_reported',palette=\"bright\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "In all the auto make cases the fraud report is almost same.\n",
    "\n",
    "# Comparing  Vehicle_Age and fraud_reported\n",
    "sns.stripplot(y='Vehicle_Age',x='fraud_reported',data=df,palette=\"bright\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "There is no significant difference between the features.\n",
    "\n",
    "Multivariate Analysis\n",
    "sns.pairplot(df,hue=\"fraud_reported\",palette=\"husl\")\n",
    "plt.show()\n",
    "\n",
    "Obervation:\n",
    "\n",
    "The pairplot gives the pairwise relation between the features on the basis of the target \"fraud_reported\". On the diagonal we can notice the distribution plots.\n",
    "From the pair plot we can observe some of the features have strong correlation with each other.\n",
    "We can also find some outliers presentin the data, we will remove them using either Zscore or IQR method.\n",
    "Identifying the outliers\n",
    "# Let's check the outliers by ploting box plot\n",
    "\n",
    "plt.figure(figsize=(25,35),facecolor='white')\n",
    "plotnumber=1\n",
    "for column in numerical_col:\n",
    "    if plotnumber<=23:\n",
    "        ax=plt.subplot(6,4,plotnumber)\n",
    "        sns.boxplot(df[column],palette=\"Set2_r\")\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "\n",
    "We can find the outliers in the following columns:\n",
    "\n",
    "age\n",
    "policy_annual_premium\n",
    "total_claim_amount\n",
    "property_claim\n",
    "incident_month\n",
    "These are the numerical columns which contains outliers. Let's remove outliers in these columns using either Zscore method or IQR method.\n",
    "\n",
    "Removing outliers\n",
    "1.Zscore method\n",
    "# Feature containing outliers\n",
    "features = df[['age','policy_annual_premium','total_claim_amount','property_claim','incident_Month']]\n",
    "\n",
    "# Using zscore to remove outliers\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z=np.abs(zscore(features))\n",
    "\n",
    "z\n",
    "age\tpolicy_annual_premium\ttotal_claim_amount\tproperty_claim\tincident_Month\n",
    "0\t0.990836\t0.616705\t0.714257\t1.165505\t0.735040\n",
    "1\t0.334073\t0.242521\t1.807312\t1.372696\t0.735040\n",
    "2\t1.088913\t0.642233\t0.686362\t0.736072\t0.429664\n",
    "3\t0.224613\t0.652886\t0.403135\t0.219722\t2.013343\n",
    "4\t0.552994\t1.341980\t1.753121\t1.399654\t0.429664\n",
    "...\t...\t...\t...\t...\t...\n",
    "995\t0.103769\t0.222884\t1.305049\t0.273817\t0.429664\n",
    "996\t0.224613\t0.739141\t2.111466\t2.214794\t0.735040\n",
    "997\t0.541611\t0.520739\t0.558507\t0.020826\t0.735040\n",
    "998\t2.523284\t0.411866\t0.219110\t0.451976\t0.429664\n",
    "999\t2.304363\t2.008710\t1.807691\t1.343664\t0.429664\n",
    "1000 rows Ã— 5 columns\n",
    "\n",
    "Now we have removed the outliers.\n",
    "\n",
    "# Creating new dataframe\n",
    "new_df = df[(z<3).all(axis=1)] \n",
    "new_df\n",
    "months_as_customer\tage\tpolicy_state\tpolicy_deductable\tpolicy_annual_premium\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tcapital-gains\tcapital-loss\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tproperty_damage\tbodily_injuries\twitnesses\tpolice_report_available\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tauto_make\tauto_model\tfraud_reported\tpolicy_bind_Day\tpolicy_bind_Month\tpolicy_bind_Year\tincident_Day\tincident_Month\tcsl_per_person\tcsl_per_accident\tVehicle_Age\n",
    "0\t328\t48\tOH\t1000\t1406.91\tMALE\tMD\tcraft-repair\tsleeping\thusband\t53300\t0\tSingle Vehicle Collision\tSide Collision\tMajor Damage\tPolice\tSC\tColumbus\t5\t1\tYES\t1\t2\tYES\t71610\t6510\t13020\t52080\tSaab\t92x\tY\t17\t10\t2014\t25\t1\t250\t500\t14\n",
    "1\t228\t42\tIN\t2000\t1197.22\tMALE\tMD\tmachine-op-inspct\treading\tother-relative\t0\t0\tVehicle Theft\tRear Collision\tMinor Damage\tPolice\tVA\tRiverwood\t8\t1\tNO\t0\t0\tNO\t5070\t780\t780\t3510\tMercedes\tE400\tY\t27\t6\t2006\t21\t1\t250\t500\t11\n",
    "2\t134\t29\tOH\t2000\t1413.14\tFEMALE\tPhD\tsales\tboard-games\town-child\t35100\t0\tMulti-vehicle Collision\tRear Collision\tMinor Damage\tPolice\tNY\tColumbus\t7\t3\tNO\t2\t3\tNO\t34650\t7700\t3850\t23100\tDodge\tRAM\tN\t9\t6\t2000\t22\t2\t100\t300\t11\n",
    "3\t256\t41\tIL\t2000\t1415.74\tFEMALE\tPhD\tarmed-forces\tboard-games\tunmarried\t48900\t-62400\tSingle Vehicle Collision\tFront Collision\tMajor Damage\tPolice\tOH\tArlington\t5\t1\tNO\t1\t2\tNO\t63400\t6340\t6340\t50720\tChevrolet\tTahoe\tY\t25\t5\t1990\t1\t10\t250\t500\t4\n",
    "4\t228\t44\tIL\t1000\t1583.91\tMALE\tAssociate\tsales\tboard-games\tunmarried\t66000\t-46000\tVehicle Theft\tRear Collision\tMinor Damage\tNone\tNY\tArlington\t20\t1\tNO\t0\t1\tNO\t6500\t1300\t650\t4550\tAccura\tRSX\tN\t6\t6\t2014\t17\t2\t500\t1000\t9\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "995\t3\t38\tOH\t1000\t1310.80\tFEMALE\tMasters\tcraft-repair\tpaintball\tunmarried\t0\t0\tSingle Vehicle Collision\tFront Collision\tMinor Damage\tFire\tNC\tNorthbrook\t20\t1\tYES\t0\t1\tNO\t87200\t17440\t8720\t61040\tHonda\tAccord\tN\t16\t7\t1991\t22\t2\t500\t1000\t12\n",
    "996\t285\t41\tIL\t1000\t1436.79\tFEMALE\tPhD\tprof-specialty\tsleeping\twife\t70900\t0\tSingle Vehicle Collision\tRear Collision\tMajor Damage\tFire\tSC\tNorthbend\t23\t1\tYES\t2\t3\tNO\t108480\t18080\t18080\t72320\tVolkswagen\tPassat\tN\t1\t5\t2014\t24\t1\t100\t300\t3\n",
    "997\t130\t34\tOH\t500\t1383.49\tFEMALE\tMasters\tarmed-forces\tbungie-jumping\tother-relative\t35100\t0\tMulti-vehicle Collision\tSide Collision\tMinor Damage\tPolice\tNC\tArlington\t4\t3\tNO\t2\t3\tYES\t67500\t7500\t7500\t52500\tSuburu\tImpreza\tN\t17\t2\t2003\t23\t1\t250\t500\t22\n",
    "998\t458\t62\tIL\t2000\t1356.92\tMALE\tAssociate\thandlers-cleaners\tbase-jumping\twife\t0\t0\tSingle Vehicle Collision\tRear Collision\tMajor Damage\tOther\tNY\tArlington\t2\t1\tNO\t0\t1\tYES\t46980\t5220\t5220\t36540\tAudi\tA5\tN\t18\t11\t2011\t26\t2\t500\t1000\t20\n",
    "999\t456\t60\tOH\t1000\t766.19\tFEMALE\tAssociate\tsales\tkayaking\thusband\t0\t0\tParked Car\tRear Collision\tMinor Damage\tPolice\tWV\tColumbus\t6\t1\tNO\t0\t3\tNO\t5060\t460\t920\t3680\tMercedes\tE400\tN\t11\t11\t1996\t26\t2\t250\t500\t11\n",
    "996 rows Ã— 39 columns\n",
    "\n",
    "This is the new dataframe after removing the outliers. Here we have removed the outliers whose Zscore is less than 3.\n",
    "\n",
    "# Shape of original dataset\n",
    "df.shape\n",
    "(1000, 39)\n",
    "Before removing outliers we had 1000 rows and 39 columns.\n",
    "\n",
    "# Shape of new dataframe\n",
    "new_df.shape\n",
    "(996, 39)\n",
    "After removing outliers there are 996 rows and 39 columns.\n",
    "\n",
    "# Checking the the data loss\n",
    "data_loss = (1000-996)/1000*100\n",
    "data_loss\n",
    "0.4\n",
    "Here we are losing very less data hence removing outliers.\n",
    "\n",
    "Let's remove the outliers and check data loss using IQR method.\n",
    "\n",
    "2. IQR (Inter Quantile Range) method\n",
    "# 1st quantile\n",
    "Q1=features.quantile(0.25)\n",
    "\n",
    "# 3rd quantile\n",
    "Q3=features.quantile(0.75)\n",
    "\n",
    "# IQR\n",
    "IQR=Q3 - Q1\n",
    "\n",
    "df1=df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df1.shape\n",
    "(942, 39)\n",
    "Using IQR method the dataframe has 942 rows and 39 columns.\n",
    "\n",
    "# Checking the the data loss\n",
    "data_loss = (1000-942)/1000*100\n",
    "data_loss\n",
    "5.800000000000001\n",
    "Using IQR method also the data loss is very less. But compared to Zscore the data loss is high in IQR, so let's consider Zscore method only.\n",
    "\n",
    "Checking skewness in the data\n",
    "# Checking the skewness\n",
    "new_df.skew()\n",
    "months_as_customer             0.359605\n",
    "age                            0.474526\n",
    "policy_deductable              0.473229\n",
    "policy_annual_premium          0.032042\n",
    "capital-gains                  0.478850\n",
    "capital-loss                  -0.393015\n",
    "incident_hour_of_the_day      -0.039123\n",
    "number_of_vehicles_involved    0.500364\n",
    "bodily_injuries                0.011117\n",
    "witnesses                      0.025758\n",
    "total_claim_amount            -0.593473\n",
    "injury_claim                   0.267970\n",
    "property_claim                 0.357130\n",
    "vehicle_claim                 -0.619755\n",
    "policy_bind_Day                0.028923\n",
    "policy_bind_Month             -0.029722\n",
    "policy_bind_Year               0.058499\n",
    "incident_Day                   0.055659\n",
    "incident_Month                 1.377097\n",
    "csl_per_person                 0.413713\n",
    "csl_per_accident               0.609316\n",
    "Vehicle_Age                    0.049276\n",
    "dtype: float64\n",
    "The following features contains the skewness:\n",
    "\n",
    "total_claim_amount\n",
    "vehicle_claim\n",
    "incident_Month\n",
    "csl_per_accident\n",
    "Removing Skewness using yeo-johnson method\n",
    "# Removing skewness using yeo-johnson  method to get better prediction\n",
    "skew = [\"total_claim_amount\",\"vehicle_claim\",\"incident_Month\",\"csl_per_accident\"]\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "'''\n",
    "parameters:\n",
    "method = 'box-cox' or 'yeo-johnson'\n",
    "'''\n",
    "\"\\nparameters:\\nmethod = 'box-cox' or 'yeo-johnson'\\n\"\n",
    "new_df[skew] = scaler.fit_transform(new_df[skew].values)\n",
    "new_df[skew].head()\n",
    "total_claim_amount\tvehicle_claim\tincident_Month\tcsl_per_accident\n",
    "0\t0.717556\t0.754553\t-1.101370\t0.052612\n",
    "1\t-1.777785\t-1.787353\t-1.101370\t0.052612\n",
    "2\t-0.716483\t-0.820820\t-0.026479\t-1.174021\n",
    "3\t0.392931\t0.678427\t1.553647\t0.052612\n",
    "4\t-1.730555\t-1.740710\t-0.026479\t1.313327\n",
    "# Checking skewness after using yeo-johnson ethod\n",
    "new_df[skew].skew()\n",
    "total_claim_amount   -0.508953\n",
    "vehicle_claim        -0.521354\n",
    "incident_Month        0.305741\n",
    "csl_per_accident      0.110964\n",
    "dtype: float64\n",
    "The skewness is almost reduced in all the columns.\n",
    "\n",
    "# After removing skewness let's check how the data has been distributed in each column.\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in new_df[skew]:\n",
    "    if plotnumber<=4:\n",
    "        ax = plt.subplot(2,2,plotnumber)\n",
    "        sns.distplot(new_df[column],color='indigo',kde_kws={\"shade\": True},hist=False)\n",
    "        plt.xlabel(column,fontsize=15)\n",
    "    plotnumber+=1\n",
    "plt.show()\n",
    "\n",
    "The data looks almost normal after removing the skewness compared to the previous data.\n",
    "\n",
    "Encoding the categorical columns using Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE=LabelEncoder()\n",
    "new_df[categorical_col]= new_df[categorical_col].apply(LE.fit_transform)\n",
    "Encoded the categorical columns using label encoder.\n",
    "\n",
    "new_df[categorical_col].head()\n",
    "policy_state\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tproperty_damage\tpolice_report_available\tauto_make\tauto_model\tfraud_reported\n",
    "0\t2\t1\t4\t2\t17\t0\t2\t2\t0\t4\t4\t1\t1\t1\t10\t1\t1\n",
    "1\t1\t1\t4\t6\t15\t2\t3\t1\t1\t4\t5\t5\t0\t0\t8\t12\t1\n",
    "2\t2\t0\t6\t11\t2\t3\t0\t1\t1\t4\t1\t1\t0\t0\t4\t30\t0\n",
    "3\t0\t0\t6\t1\t2\t4\t2\t0\t0\t4\t2\t0\t0\t0\t3\t34\t1\n",
    "4\t0\t1\t0\t11\t2\t4\t3\t1\t1\t2\t1\t0\t0\t0\t0\t31\t0\n",
    "The categorical columns have been converted into numerical columns by using label encoding.\n",
    "\n",
    "Correlation between the target variable and independent variables using HEAT map\n",
    "# Checking the correlation between features and the target\n",
    "cor = new_df.corr()\n",
    "cor\n",
    "months_as_customer\tage\tpolicy_state\tpolicy_deductable\tpolicy_annual_premium\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tcapital-gains\tcapital-loss\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tproperty_damage\tbodily_injuries\twitnesses\tpolice_report_available\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tauto_make\tauto_model\tfraud_reported\tpolicy_bind_Day\tpolicy_bind_Month\tpolicy_bind_Year\tincident_Day\tincident_Month\tcsl_per_person\tcsl_per_accident\tVehicle_Age\n",
    "months_as_customer\t1.000000\t0.922092\t0.000118\t0.023512\t-0.003378\t0.059002\t-0.000848\t0.005365\t-0.092575\t0.071097\t0.007601\t0.020771\t-0.017202\t-0.072479\t-0.061932\t0.011406\t0.006906\t-0.001837\t0.068962\t0.013609\t0.002663\t-0.011126\t0.063530\t-0.018760\t0.064662\t0.066416\t0.041157\t0.062392\t0.049222\t0.002908\t0.020966\t0.054175\t0.004955\t-0.046010\t-0.013502\t0.019029\t-0.022063\t-0.022928\t-0.001287\n",
    "age\t0.922092\t1.000000\t-0.015182\t0.026772\t0.005890\t0.072900\t0.001485\t0.014030\t-0.080587\t0.075940\t-0.006333\t0.008192\t-0.025979\t-0.075892\t-0.060361\t0.016000\t0.007053\t-0.004009\t0.085509\t0.021140\t-0.005795\t-0.017461\t0.056658\t-0.018493\t0.070582\t0.075480\t0.064230\t0.062165\t0.032105\t0.016258\t0.012408\t0.054465\t0.015874\t-0.035505\t-0.008174\t0.016685\t-0.012844\t-0.012459\t-0.003268\n",
    "policy_state\t0.000118\t-0.015182\t1.000000\t0.010740\t0.014475\t-0.019294\t-0.032551\t-0.031016\t-0.040215\t-0.018855\t0.072552\t-0.032268\t0.031769\t0.050953\t-0.026728\t-0.017738\t0.009195\t-0.004475\t0.026464\t-0.043447\t-0.009334\t0.008992\t0.024809\t0.065957\t-0.008788\t-0.046398\t0.002678\t-0.000472\t0.015848\t-0.024513\t0.028864\t0.001030\t0.071527\t-0.005709\t0.013370\t0.026233\t-0.005866\t-0.000653\t-0.002323\n",
    "policy_deductable\t0.023512\t0.026772\t0.010740\t1.000000\t-0.008445\t-0.011671\t0.014571\t-0.050024\t0.005134\t-0.021763\t0.035950\t-0.024018\t-0.038696\t-0.054153\t-0.017352\t-0.000210\t0.007419\t-0.046560\t0.059733\t0.051317\t0.014131\t-0.023384\t0.069433\t0.040969\t0.025169\t0.040337\t0.069537\t0.005902\t-0.034266\t-0.003597\t0.015833\t0.011204\t-0.051368\t-0.048465\t-0.018867\t-0.000169\t0.006634\t0.006179\t-0.027497\n",
    "policy_annual_premium\t-0.003378\t0.005890\t0.014475\t-0.008445\t1.000000\t0.041830\t-0.021475\t0.030963\t-0.014376\t0.006474\t-0.009757\t0.029835\t0.049336\t0.034999\t-0.015788\t-0.058852\t0.049047\t0.047389\t-0.003001\t-0.044711\t0.078922\t0.025323\t0.006365\t0.029619\t0.002905\t-0.024031\t-0.007964\t0.012961\t0.011595\t-0.034998\t-0.010174\t-0.030886\t-0.028452\t0.001650\t0.016400\t-0.012386\t0.028569\t0.030755\t0.040439\n",
    "insured_sex\t0.059002\t0.072900\t-0.019294\t-0.011671\t0.041830\t1.000000\t0.007473\t-0.009257\t-0.019600\t0.004104\t-0.020651\t-0.024440\t0.010572\t-0.014399\t-0.029576\t0.019000\t-0.021822\t0.013251\t0.017950\t0.019275\t0.025911\t-0.020201\t0.044173\t0.009884\t-0.020117\t0.004603\t-0.009457\t-0.026816\t0.026040\t-0.017679\t0.032173\t0.045694\t-0.007259\t-0.031981\t-0.007249\t0.024223\t-0.019001\t-0.008959\t0.028638\n",
    "insured_education_level\t-0.000848\t0.001485\t-0.032551\t0.014571\t-0.021475\t0.007473\t1.000000\t-0.021502\t0.022806\t-0.021595\t0.040283\t0.039890\t0.012385\t-0.020687\t-0.007395\t-0.011831\t-0.006118\t-0.019160\t-0.057232\t-0.001308\t-0.048624\t-0.011455\t0.017577\t0.054380\t0.076528\t0.083672\t0.058663\t0.070001\t0.057528\t0.050878\t0.011520\t0.014464\t-0.007489\t0.017801\t-0.055411\t0.069312\t0.011093\t0.010425\t-0.045094\n",
    "insured_occupation\t0.005365\t0.014030\t-0.031016\t-0.050024\t0.030963\t-0.009257\t-0.021502\t1.000000\t-0.009818\t-0.008141\t-0.007553\t-0.021993\t0.003427\t0.006634\t0.002255\t-0.007367\t-0.021531\t-0.002158\t-0.021308\t-0.015177\t-0.018839\t-0.001830\t-0.011373\t-0.021730\t0.006458\t0.022693\t0.008301\t0.000954\t0.044321\t0.020878\t0.003335\t-0.003030\t0.024516\t-0.025282\t-0.022786\t0.013226\t0.018978\t0.019575\t0.004827\n",
    "insured_hobbies\t-0.092575\t-0.080587\t-0.040215\t0.005134\t-0.014376\t-0.019600\t0.022806\t-0.009818\t1.000000\t-0.097837\t0.053503\t-0.032345\t0.008256\t0.016756\t-0.023999\t-0.028153\t0.085633\t0.018605\t0.019802\t0.011927\t0.036599\t0.030730\t-0.034308\t-0.030149\t-0.003759\t0.028415\t0.009925\t-0.015968\t-0.000055\t0.031208\t-0.047983\t0.028387\t-0.005233\t-0.013819\t0.014295\t-0.001239\t-0.013322\t-0.007772\t0.015606\n",
    "insured_relationship\t0.071097\t0.075940\t-0.018855\t-0.021763\t0.006474\t0.004104\t-0.021595\t-0.008141\t-0.097837\t1.000000\t-0.019234\t-0.062730\t0.035930\t-0.003383\t0.017329\t-0.002501\t-0.062708\t-0.000285\t0.006576\t-0.049865\t-0.031474\t0.000356\t0.002671\t-0.019792\t0.002431\t0.027760\t0.006420\t-0.005901\t-0.029954\t0.062286\t0.017992\t0.008900\t0.040931\t-0.006190\t-0.083525\t0.039255\t-0.022125\t-0.031298\t0.026099\n",
    "capital-gains\t0.007601\t-0.006333\t0.072552\t0.035950\t-0.009757\t-0.020651\t0.040283\t-0.007553\t0.053503\t-0.019234\t1.000000\t-0.049528\t-0.046159\t-0.007727\t-0.013583\t-0.045766\t0.013865\t-0.026616\t-0.018367\t0.062047\t-0.000648\t0.053608\t-0.017785\t-0.014806\t0.015032\t0.027397\t-0.003947\t0.015089\t0.051085\t0.038062\t-0.018919\t0.008550\t-0.012221\t0.041789\t0.015736\t0.039008\t0.003241\t0.004182\t-0.031282\n",
    "capital-loss\t0.020771\t0.008192\t-0.032268\t-0.024018\t0.029835\t-0.024440\t0.039890\t-0.021993\t-0.032345\t-0.062730\t-0.049528\t1.000000\t0.025427\t0.038674\t-0.020159\t0.024119\t-0.048525\t-0.041473\t-0.028729\t-0.013205\t0.016536\t-0.027828\t-0.041611\t-0.039428\t-0.034401\t-0.041919\t-0.023884\t-0.030884\t-0.042534\t-0.034395\t-0.012552\t-0.034728\t0.000313\t-0.025891\t-0.013323\t0.034281\t-0.032792\t-0.034327\t0.056410\n",
    "incident_type\t-0.017202\t-0.025979\t0.031769\t-0.038696\t0.049336\t0.010572\t0.012385\t0.003427\t0.008256\t0.035930\t-0.046159\t0.025427\t1.000000\t-0.073681\t0.159142\t0.098504\t0.037034\t0.020880\t-0.134771\t-0.893459\t0.020495\t-0.011299\t-0.008705\t0.020878\t-0.272780\t-0.227878\t-0.225052\t-0.266138\t-0.029759\t-0.033110\t-0.047850\t-0.009855\t0.037692\t-0.050534\t0.015950\t-0.025874\t0.012625\t0.012430\t0.050546\n",
    "collision_type\t-0.072479\t-0.075892\t0.050953\t-0.054153\t0.034999\t-0.014399\t-0.020687\t0.006634\t0.016756\t-0.003383\t-0.007727\t0.038674\t-0.073681\t1.000000\t-0.016266\t0.059729\t0.031791\t0.029459\t0.041586\t0.081949\t0.003990\t-0.021677\t-0.047988\t0.047914\t-0.013927\t-0.023925\t-0.007171\t-0.011090\t-0.005133\t0.026855\t-0.014184\t0.018971\t-0.039633\t0.025696\t0.050464\t-0.050068\t0.021872\t0.020870\t-0.001578\n",
    "incident_severity\t-0.061932\t-0.060361\t-0.026728\t-0.017352\t-0.015788\t-0.029576\t-0.007395\t0.002255\t-0.023999\t0.017329\t-0.013583\t-0.020159\t0.159142\t-0.016266\t1.000000\t0.164659\t0.060738\t-0.018606\t-0.074242\t-0.168193\t-0.037387\t-0.020712\t-0.009772\t0.004715\t-0.360791\t-0.276638\t-0.302720\t-0.356913\t-0.006851\t-0.045812\t-0.405287\t-0.039983\t-0.033449\t0.005061\t0.018603\t0.001793\t0.022052\t0.024400\t0.017728\n",
    "authorities_contacted\t0.011406\t0.016000\t-0.017738\t-0.000210\t-0.058852\t0.019000\t-0.011831\t-0.007367\t-0.028153\t-0.002501\t-0.045766\t0.024119\t0.098504\t0.059729\t0.164659\t1.000000\t0.026272\t-0.004550\t-0.062119\t-0.079519\t-0.004318\t0.039991\t-0.005618\t0.044226\t-0.167377\t-0.142654\t-0.123294\t-0.166815\t-0.028113\t0.019139\t-0.043676\t-0.036122\t-0.051774\t-0.028862\t-0.010427\t0.015956\t0.050414\t0.055484\t-0.074965\n",
    "incident_state\t0.006906\t0.007053\t0.009195\t0.007419\t0.049047\t-0.021822\t-0.006118\t-0.021531\t0.085633\t-0.062708\t0.013865\t-0.048525\t0.037034\t0.031791\t0.060738\t0.026272\t1.000000\t0.015459\t-0.035569\t-0.036985\t-0.044690\t0.000814\t0.010604\t0.044570\t-0.042679\t-0.016281\t-0.028150\t-0.049182\t0.070313\t-0.039711\t-0.051534\t0.035439\t0.004657\t-0.026345\t0.018189\t-0.022515\t0.028384\t0.031736\t0.015079\n",
    "incident_city\t-0.001837\t-0.004009\t-0.004475\t-0.046560\t0.047389\t0.013251\t-0.019160\t-0.002158\t0.018605\t-0.000285\t-0.026616\t-0.041473\t0.020880\t0.029459\t-0.018606\t-0.004550\t0.015459\t1.000000\t0.013175\t-0.008984\t0.008329\t0.047886\t0.011056\t0.010681\t0.043218\t0.048059\t0.040420\t0.037167\t-0.001242\t0.016819\t-0.038545\t0.021935\t0.066707\t-0.008944\t-0.050710\t0.036258\t0.013975\t0.010884\t-0.060932\n",
    "incident_hour_of_the_day\t0.068962\t0.085509\t0.026464\t0.059733\t-0.003001\t0.017950\t-0.057232\t-0.021308\t0.019802\t0.006576\t-0.018367\t-0.028729\t-0.134771\t0.041586\t-0.074242\t-0.062119\t-0.035569\t0.013175\t1.000000\t0.121555\t0.062666\t-0.039539\t0.009373\t0.040709\t0.215756\t0.168915\t0.179917\t0.212529\t-0.002292\t-0.056941\t0.005863\t-0.007718\t-0.002553\t0.037596\t-0.028480\t0.031384\t-0.001586\t-0.000614\t-0.023178\n",
    "number_of_vehicles_involved\t0.013609\t0.021140\t-0.043447\t0.051317\t-0.044711\t0.019275\t-0.001308\t-0.015177\t0.011927\t-0.049865\t0.062047\t-0.013205\t-0.893459\t0.081949\t-0.168193\t-0.079519\t-0.036985\t-0.008984\t0.121555\t1.000000\t-0.014557\t0.014493\t-0.011877\t-0.008909\t0.268557\t0.224700\t0.221560\t0.261934\t0.011417\t0.030305\t0.048612\t0.005426\t-0.045481\t0.028498\t-0.022825\t0.015878\t-0.019306\t-0.019764\t-0.032343\n",
    "property_damage\t0.002663\t-0.005795\t-0.009334\t0.014131\t0.078922\t0.025911\t-0.048624\t-0.018839\t0.036599\t-0.031474\t-0.000648\t0.016536\t0.020495\t0.003990\t-0.037387\t-0.004318\t-0.044690\t0.008329\t0.062666\t-0.014557\t1.000000\t0.002185\t-0.016813\t-0.021675\t0.062703\t0.049817\t0.047484\t0.062581\t-0.010105\t-0.051946\t0.017270\t-0.011219\t0.007666\t0.019736\t0.007737\t-0.004096\t-0.025237\t-0.025875\t0.037093\n",
    "bodily_injuries\t-0.011126\t-0.017461\t0.008992\t-0.023384\t0.025323\t-0.020201\t-0.011455\t-0.001830\t0.030730\t0.000356\t0.053608\t-0.027828\t-0.011299\t-0.021677\t-0.020712\t0.039991\t0.000814\t0.047886\t-0.039539\t0.014493\t0.002185\t1.000000\t-0.003434\t0.010255\t0.046994\t0.048767\t0.035347\t0.043559\t0.027461\t0.021894\t0.035484\t-0.043902\t-0.031687\t0.031887\t-0.032359\t0.049748\t0.001772\t-0.000345\t0.018437\n",
    "witnesses\t0.063530\t0.056658\t0.024809\t0.069433\t0.006365\t0.044173\t0.017577\t-0.011373\t-0.034308\t0.002671\t-0.017785\t-0.041611\t-0.008705\t-0.047988\t-0.009772\t-0.005618\t0.010604\t0.011056\t0.009373\t-0.011877\t-0.016813\t-0.003434\t1.000000\t0.048059\t-0.012244\t-0.025750\t0.050073\t-0.023812\t0.006805\t-0.009799\t0.051283\t0.003458\t-0.014507\t0.052651\t0.017472\t-0.017142\t0.065395\t0.069088\t-0.046001\n",
    "police_report_available\t-0.018760\t-0.018493\t0.065957\t0.040969\t0.029619\t0.009884\t0.054380\t-0.021730\t-0.030149\t-0.019792\t-0.014806\t-0.039428\t0.020878\t0.047914\t0.004715\t0.044226\t0.044570\t0.010681\t0.040709\t-0.008909\t-0.021675\t0.010255\t0.048059\t1.000000\t0.032962\t0.026952\t0.005117\t0.038220\t0.065478\t-0.017911\t-0.030419\t-0.030304\t0.011663\t-0.011852\t0.027983\t-0.007247\t-0.015259\t-0.005991\t0.024506\n",
    "total_claim_amount\t0.064662\t0.070582\t-0.008788\t0.025169\t0.002905\t-0.020117\t0.076528\t0.006458\t-0.003759\t0.002431\t0.015032\t-0.034401\t-0.272780\t-0.013927\t-0.360791\t-0.167377\t-0.042679\t0.043218\t0.215756\t0.268557\t0.062703\t0.046994\t-0.012244\t0.032962\t1.000000\t0.806404\t0.813265\t0.981240\t-0.057412\t0.040535\t0.162203\t-0.007178\t0.061122\t-0.002566\t-0.049040\t0.045931\t-0.050838\t-0.057052\t0.034282\n",
    "injury_claim\t0.066416\t0.075480\t-0.046398\t0.040337\t-0.024031\t0.004603\t0.083672\t0.022693\t0.028415\t0.027760\t0.027397\t-0.041919\t-0.227878\t-0.023925\t-0.276638\t-0.142654\t-0.016281\t0.048059\t0.168915\t0.224700\t0.049817\t0.048767\t-0.025750\t0.026952\t0.806404\t1.000000\t0.566266\t0.720163\t-0.038425\t0.036318\t0.089900\t0.010659\t0.018797\t0.001598\t-0.030751\t0.009277\t-0.074952\t-0.080216\t0.013204\n",
    "property_claim\t0.041157\t0.064230\t0.002678\t0.069537\t-0.007964\t-0.009457\t0.058663\t0.008301\t0.009925\t0.006420\t-0.003947\t-0.023884\t-0.225052\t-0.007171\t-0.302720\t-0.123294\t-0.028150\t0.040420\t0.179917\t0.221560\t0.047484\t0.035347\t0.050073\t0.005117\t0.813265\t0.566266\t1.000000\t0.731656\t-0.043044\t0.051063\t0.138572\t-0.010225\t0.057868\t-0.002113\t-0.023208\t0.021474\t-0.042894\t-0.047923\t0.014117\n",
    "vehicle_claim\t0.062392\t0.062165\t-0.000472\t0.005902\t0.012961\t-0.026816\t0.070001\t0.000954\t-0.015968\t-0.005901\t0.015089\t-0.030884\t-0.266138\t-0.011090\t-0.356913\t-0.166815\t-0.049182\t0.037167\t0.212529\t0.261934\t0.062581\t0.043559\t-0.023812\t0.038220\t0.981240\t0.720163\t0.731656\t1.000000\t-0.059004\t0.034047\t0.168862\t-0.010329\t0.066016\t-0.003297\t-0.055051\t0.057173\t-0.040517\t-0.046567\t0.041049\n",
    "auto_make\t0.049222\t0.032105\t0.015848\t-0.034266\t0.011595\t0.026040\t0.057528\t0.044321\t-0.000055\t-0.029954\t0.051085\t-0.042534\t-0.029759\t-0.005133\t-0.006851\t-0.028113\t0.070313\t-0.001242\t-0.002292\t0.011417\t-0.010105\t0.027461\t0.006805\t0.065478\t-0.057412\t-0.038425\t-0.043044\t-0.059004\t1.000000\t-0.167538\t-0.031261\t-0.014279\t-0.044374\t-0.003427\t0.054629\t-0.027293\t-0.001119\t-0.006269\t-0.000649\n",
    "auto_model\t0.002908\t0.016258\t-0.024513\t-0.003597\t-0.034998\t-0.017679\t0.050878\t0.020878\t0.031208\t0.062286\t0.038062\t-0.034395\t-0.033110\t0.026855\t-0.045812\t0.019139\t-0.039711\t0.016819\t-0.056941\t0.030305\t-0.051946\t0.021894\t-0.009799\t-0.017911\t0.040535\t0.036318\t0.051063\t0.034047\t-0.167538\t1.000000\t-0.001416\t-0.011989\t-0.003534\t0.027615\t-0.043232\t0.013615\t0.045455\t0.048219\t-0.031997\n",
    "fraud_reported\t0.020966\t0.012408\t0.028864\t0.015833\t-0.010174\t0.032173\t0.011520\t0.003335\t-0.047983\t0.017992\t-0.018919\t-0.012552\t-0.047850\t-0.014184\t-0.405287\t-0.043676\t-0.051534\t-0.038545\t0.005863\t0.048612\t0.017270\t0.035484\t0.051283\t-0.030419\t0.162203\t0.089900\t0.138572\t0.168862\t-0.031261\t-0.001416\t1.000000\t0.059756\t-0.031288\t0.001029\t-0.048187\t-0.007073\t-0.042820\t-0.039039\t-0.004758\n",
    "policy_bind_Day\t0.054175\t0.054465\t0.001030\t0.011204\t-0.030886\t0.045694\t0.014464\t-0.003030\t0.028387\t0.008900\t0.008550\t-0.034728\t-0.009855\t0.018971\t-0.039983\t-0.036122\t0.035439\t0.021935\t-0.007718\t0.005426\t-0.011219\t-0.043902\t0.003458\t-0.030304\t-0.007178\t0.010659\t-0.010225\t-0.010329\t-0.014279\t-0.011989\t0.059756\t1.000000\t0.036360\t-0.001053\t0.022132\t0.020812\t0.012794\t0.016335\t-0.026429\n",
    "policy_bind_Month\t0.004955\t0.015874\t0.071527\t-0.051368\t-0.028452\t-0.007259\t-0.007489\t0.024516\t-0.005233\t0.040931\t-0.012221\t0.000313\t0.037692\t-0.039633\t-0.033449\t-0.051774\t0.004657\t0.066707\t-0.002553\t-0.045481\t0.007666\t-0.031687\t-0.014507\t0.011663\t0.061122\t0.018797\t0.057868\t0.066016\t-0.044374\t-0.003534\t-0.031288\t0.036360\t1.000000\t-0.008801\t-0.043049\t0.038988\t-0.045978\t-0.044751\t0.004436\n",
    "policy_bind_Year\t-0.046010\t-0.035505\t-0.005709\t-0.048465\t0.001650\t-0.031981\t0.017801\t-0.025282\t-0.013819\t-0.006190\t0.041789\t-0.025891\t-0.050534\t0.025696\t0.005061\t-0.028862\t-0.026345\t-0.008944\t0.037596\t0.028498\t0.019736\t0.031887\t0.052651\t-0.011852\t-0.002566\t0.001598\t-0.002113\t-0.003297\t-0.003427\t0.027615\t0.001029\t-0.001053\t-0.008801\t1.000000\t0.052418\t-0.054315\t-0.016414\t-0.017605\t-0.015600\n",
    "incident_Day\t-0.013502\t-0.008174\t0.013370\t-0.018867\t0.016400\t-0.007249\t-0.055411\t-0.022786\t0.014295\t-0.083525\t0.015736\t-0.013323\t0.015950\t0.050464\t0.018603\t-0.010427\t0.018189\t-0.050710\t-0.028480\t-0.022825\t0.007737\t-0.032359\t0.017472\t0.027983\t-0.049040\t-0.030751\t-0.023208\t-0.055051\t0.054629\t-0.043232\t-0.048187\t0.022132\t-0.043049\t0.052418\t1.000000\t-0.690452\t-0.007701\t-0.004092\t-0.013885\n",
    "incident_Month\t0.019029\t0.016685\t0.026233\t-0.000169\t-0.012386\t0.024223\t0.069312\t0.013226\t-0.001239\t0.039255\t0.039008\t0.034281\t-0.025874\t-0.050068\t0.001793\t0.015956\t-0.022515\t0.036258\t0.031384\t0.015878\t-0.004096\t0.049748\t-0.017142\t-0.007247\t0.045931\t0.009277\t0.021474\t0.057173\t-0.027293\t0.013615\t-0.007073\t0.020812\t0.038988\t-0.054315\t-0.690452\t1.000000\t-0.038723\t-0.043496\t0.019706\n",
    "csl_per_person\t-0.022063\t-0.012844\t-0.005866\t0.006634\t0.028569\t-0.019001\t0.011093\t0.018978\t-0.013322\t-0.022125\t0.003241\t-0.032792\t0.012625\t0.021872\t0.022052\t0.050414\t0.028384\t0.013975\t-0.001586\t-0.019306\t-0.025237\t0.001772\t0.065395\t-0.015259\t-0.050838\t-0.074952\t-0.042894\t-0.040517\t-0.001119\t0.045455\t-0.042820\t0.012794\t-0.045978\t-0.016414\t-0.007701\t-0.038723\t1.000000\t0.990224\t0.037820\n",
    "csl_per_accident\t-0.022928\t-0.012459\t-0.000653\t0.006179\t0.030755\t-0.008959\t0.010425\t0.019575\t-0.007772\t-0.031298\t0.004182\t-0.034327\t0.012430\t0.020870\t0.024400\t0.055484\t0.031736\t0.010884\t-0.000614\t-0.019764\t-0.025875\t-0.000345\t0.069088\t-0.005991\t-0.057052\t-0.080216\t-0.047923\t-0.046567\t-0.006269\t0.048219\t-0.039039\t0.016335\t-0.044751\t-0.017605\t-0.004092\t-0.043496\t0.990224\t1.000000\t0.035074\n",
    "Vehicle_Age\t-0.001287\t-0.003268\t-0.002323\t-0.027497\t0.040439\t0.028638\t-0.045094\t0.004827\t0.015606\t0.026099\t-0.031282\t0.056410\t0.050546\t-0.001578\t0.017728\t-0.074965\t0.015079\t-0.060932\t-0.023178\t-0.032343\t0.037093\t0.018437\t-0.046001\t0.024506\t0.034282\t0.013204\t0.014117\t0.041049\t-0.000649\t-0.031997\t-0.004758\t-0.026429\t0.004436\t-0.015600\t-0.013885\t0.019706\t0.037820\t0.035074\t1.000000\n",
    "This gives the correlation between the denpendent and independent variables. We can visualize this by plotting heat map.\n",
    "\n",
    "# Visualizing the correlation matrix by plotting heat map.\n",
    "plt.figure(figsize=(30,25))\n",
    "sns.heatmap(new_df.corr(),linewidths=.1,vmin=-1, vmax=1, fmt='.1g',linecolor=\"black\", annot = True, annot_kws={'size':10},cmap=\"coolwarm_r\")\n",
    "plt.yticks(rotation=0);\n",
    "\n",
    "This heatmap shows the correlation matrix by visualizing the data. we can observe the relation between one feature to other.\n",
    "\n",
    "This heat mapcontains both positive and negative correlation.\n",
    "\n",
    "There is very less correlation between the target and the label.\n",
    "We can observe the most of the columns are highly correlated with each other which leads to the multicollinearity problem.\n",
    "We will check the VIF value to overcome with this multicollinearity problem.\n",
    "Visualizing the correlation between label and features using bar plot\n",
    "plt.figure(figsize=(22,10))\n",
    "new_df.corr()['fraud_reported'].sort_values(ascending=False).drop(['fraud_reported']).plot(kind='bar',color='y')\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "plt.ylabel('Target',fontsize=14)\n",
    "plt.title('correlation between lanel and feature using bar plot',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "From the bar plot we can observe that the columns policy_bind_Year, insured_occupation and auto_model age are very less correlated with the target. We can drop this columns if necessary.\n",
    "\n",
    "Separating the features and label variables into x and y\n",
    "x = new_df.drop(\"fraud_reported\", axis=1)\n",
    "y = new_df[\"fraud_reported\"]\n",
    "We have separated both dependent and independent variables.\n",
    "\n",
    "# Dimension of x\n",
    "x.shape\n",
    "(996, 38)\n",
    "# Dimension of y\n",
    "y.shape\n",
    "(996,)\n",
    "Feature Scaling using Standard Scalarization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)\n",
    "x.head()\n",
    "months_as_customer\tage\tpolicy_state\tpolicy_deductable\tpolicy_annual_premium\tinsured_sex\tinsured_education_level\tinsured_occupation\tinsured_hobbies\tinsured_relationship\tcapital-gains\tcapital-loss\tincident_type\tcollision_type\tincident_severity\tauthorities_contacted\tincident_state\tincident_city\tincident_hour_of_the_day\tnumber_of_vehicles_involved\tproperty_damage\tbodily_injuries\twitnesses\tpolice_report_available\ttotal_claim_amount\tinjury_claim\tproperty_claim\tvehicle_claim\tauto_make\tauto_model\tpolicy_bind_Day\tpolicy_bind_Month\tpolicy_bind_Year\tincident_Day\tincident_Month\tcsl_per_person\tcsl_per_accident\tVehicle_Age\n",
    "0\t1.074671\t0.987190\t1.186130\t-0.224722\t0.621371\t1.075102\t0.531088\t-1.162296\t1.280299\t-1.436113\t1.010158\t0.953763\t0.765670\t1.343731\t-1.261019\t1.196829\t0.352556\t-0.964353\t-0.959495\t-0.824928\t1.515920\t0.007356\t0.466826\t1.480644\t0.717556\t-0.188161\t1.174449\t0.754553\t0.857248\t-1.644810\t0.177487\t1.013751\t1.686176\t1.139807\t-1.101370\t-0.137684\t0.052612\t0.182338\n",
    "1\t0.204846\t0.330455\t-0.018137\t1.409024\t-0.251375\t1.075102\t0.531088\t-0.166257\t0.928186\t-0.246617\t-0.900970\t0.953763\t1.690043\t-0.030351\t-0.197535\t1.196829\t0.815808\t0.992846\t-0.527592\t-0.824928\t-0.659665\t-1.213749\t-1.335340\t-0.675382\t-1.777785\t-1.363038\t-1.376935\t-1.787353\t0.360986\t-0.657437\t1.309223\t-0.166145\t0.599693\t0.756374\t-1.101370\t-0.137684\t0.052612\t-0.316587\n",
    "2\t-0.612790\t-1.092470\t1.186130\t1.409024\t0.647301\t-0.930144\t1.557206\t1.078792\t-1.360550\t0.348131\t0.357578\t0.953763\t-1.083075\t-0.030351\t-0.197535\t1.196829\t-1.037202\t-0.964353\t-0.671560\t1.138341\t-0.659665\t1.228462\t1.367910\t-0.675382\t-0.716483\t0.055836\t-0.737005\t-0.820820\t-0.631538\t0.958263\t-0.727901\t-0.166145\t-0.215170\t0.852233\t-0.026479\t-1.066352\t-1.174021\t-0.316587\n",
    "3\t0.448397\t0.221000\t-1.222403\t1.409024\t0.658123\t-0.930144\t1.557206\t-1.411305\t-1.360550\t0.942878\t0.852392\t-1.269246\t0.765670\t-1.404433\t-1.261019\t1.196829\t-0.573950\t-1.453652\t-0.959495\t-0.824928\t-0.659665\t0.007356\t0.466826\t-0.675382\t0.392931\t-0.223018\t-0.217973\t0.678427\t-0.879669\t1.317308\t1.082875\t-0.461119\t-1.573274\t-1.160788\t1.553647\t-0.137684\t0.052612\t-1.480744\n",
    "4\t0.204846\t0.549367\t-1.222403\t-0.224722\t1.358059\t1.075102\t-1.521148\t1.078792\t-1.360550\t0.942878\t1.465530\t-0.684994\t1.690043\t-0.030351\t-0.197535\t-0.108088\t-1.037202\t-1.453652\t1.200020\t-0.824928\t-0.659665\t-1.213749\t-0.434257\t-0.675382\t-1.730555\t-1.256417\t-1.404033\t-1.740710\t-1.624063\t1.048024\t-1.067422\t-0.166145\t1.686176\t0.372942\t-0.026479\t1.410096\t1.313327\t-0.649203\n",
    "I have scaled the data using standard scalarizaion method to overcome with the issue of data biasness.\n",
    "\n",
    "In the heat map we have found some features having high correlation between each other which means multicollinearity problem so let's check the VIF value to solve multicollinearity problem.\n",
    "\n",
    "Checking Variance Inflation Factor(VIF)\n",
    "# Finding varience inflation factor in each scaled column i.e, x.shape[1] (1/(1-R2))\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF values\"] = [variance_inflation_factor(x.values,i)\n",
    "              for i in range(len(x.columns))]\n",
    "vif[\"Features\"] = x.columns\n",
    "\n",
    "# Let's check the values\n",
    "vif\n",
    "VIF values\tFeatures\n",
    "0\t6.851319\tmonths_as_customer\n",
    "1\t6.855557\tage\n",
    "2\t1.039410\tpolicy_state\n",
    "3\t1.045656\tpolicy_deductable\n",
    "4\t1.037450\tpolicy_annual_premium\n",
    "5\t1.037410\tinsured_sex\n",
    "6\t1.047043\tinsured_education_level\n",
    "7\t1.017493\tinsured_occupation\n",
    "8\t1.052669\tinsured_hobbies\n",
    "9\t1.052965\tinsured_relationship\n",
    "10\t1.040529\tcapital-gains\n",
    "11\t1.042461\tcapital-loss\n",
    "12\t5.151758\tincident_type\n",
    "13\t1.047678\tcollision_type\n",
    "14\t1.240895\tincident_severity\n",
    "15\t1.107639\tauthorities_contacted\n",
    "16\t1.045277\tincident_state\n",
    "17\t1.030872\tincident_city\n",
    "18\t1.102947\tincident_hour_of_the_day\n",
    "19\t5.124218\tnumber_of_vehicles_involved\n",
    "20\t1.030337\tproperty_damage\n",
    "21\t1.028589\tbodily_injuries\n",
    "22\t1.044300\twitnesses\n",
    "23\t1.044785\tpolice_report_available\n",
    "24\t43288.328057\ttotal_claim_amount\n",
    "25\t1595.753768\tinjury_claim\n",
    "26\t1549.162785\tproperty_claim\n",
    "27\t21553.456819\tvehicle_claim\n",
    "28\t1.078702\tauto_make\n",
    "29\t1.066796\tauto_model\n",
    "30\t1.025460\tpolicy_bind_Day\n",
    "31\t1.039616\tpolicy_bind_Month\n",
    "32\t1.028406\tpolicy_bind_Year\n",
    "33\t1.969181\tincident_Day\n",
    "34\t1.979298\tincident_Month\n",
    "35\t52.779677\tcsl_per_person\n",
    "36\t52.918708\tcsl_per_accident\n",
    "37\t1.040959\tVehicle_Age\n",
    "We can observe some columns have VIF above 10 that means they are causing multicollinearity problem.\n",
    "\n",
    "total_claim amount\n",
    "injury_claim\n",
    "property_claim\n",
    "vehicle_claim\n",
    "csl_per_person\n",
    "csl_per_accident\n",
    "Let's drop the feature having high VIF value amongst all the columns.\n",
    "\n",
    "# Dropping total_claim_amount column as it contains high VIF value\n",
    "x.drop([\"total_claim_amount\"],axis=1,inplace=True)\n",
    "# Again checking VIF value to confirm whether the multicollinearity still exists or not\n",
    "vif=pd.DataFrame()\n",
    "vif[\"vif_Features\"]=[variance_inflation_factor(x.values, i) \n",
    "                     for i in range(x.shape[1])]\n",
    "vif[\"Features\"]=x.columns\n",
    "vif\n",
    "vif_Features\tFeatures\n",
    "0\t6.849523\tmonths_as_customer\n",
    "1\t6.855452\tage\n",
    "2\t1.039378\tpolicy_state\n",
    "3\t1.044129\tpolicy_deductable\n",
    "4\t1.036722\tpolicy_annual_premium\n",
    "5\t1.034404\tinsured_sex\n",
    "6\t1.046991\tinsured_education_level\n",
    "7\t1.016776\tinsured_occupation\n",
    "8\t1.050855\tinsured_hobbies\n",
    "9\t1.052941\tinsured_relationship\n",
    "10\t1.040477\tcapital-gains\n",
    "11\t1.042289\tcapital-loss\n",
    "12\t5.141590\tincident_type\n",
    "13\t1.047497\tcollision_type\n",
    "14\t1.195570\tincident_severity\n",
    "15\t1.085702\tauthorities_contacted\n",
    "16\t1.040880\tincident_state\n",
    "17\t1.029837\tincident_city\n",
    "18\t1.096945\tincident_hour_of_the_day\n",
    "19\t5.112157\tnumber_of_vehicles_involved\n",
    "20\t1.030127\tproperty_damage\n",
    "21\t1.026279\tbodily_injuries\n",
    "22\t1.044259\twitnesses\n",
    "23\t1.043381\tpolice_report_available\n",
    "24\t2.176141\tinjury_claim\n",
    "25\t2.253675\tproperty_claim\n",
    "26\t3.359532\tvehicle_claim\n",
    "27\t1.076002\tauto_make\n",
    "28\t1.066163\tauto_model\n",
    "29\t1.025362\tpolicy_bind_Day\n",
    "30\t1.039588\tpolicy_bind_Month\n",
    "31\t1.028184\tpolicy_bind_Year\n",
    "32\t1.968018\tincident_Day\n",
    "33\t1.973791\tincident_Month\n",
    "34\t52.768853\tcsl_per_person\n",
    "35\t52.900590\tcsl_per_accident\n",
    "36\t1.040311\tVehicle_Age\n",
    "Still we have high VIF value in csl_per_person and csl_per_accident. Let's remove csl_per_accident column as it has VIF bit greater than csl_per_person.\n",
    "\n",
    "# Dropping csl_per_accident column\n",
    "x.drop([\"csl_per_accident\"],axis=1,inplace=True)\n",
    "# Again checking VIF value to confirm whether the multicollinearity still exists or not\n",
    "vif=pd.DataFrame()\n",
    "vif[\"vif_Features\"]=[variance_inflation_factor(x.values, i) \n",
    "                     for i in range(x.shape[1])]\n",
    "vif[\"Features\"]=x.columns\n",
    "vif\n",
    "vif_Features\tFeatures\n",
    "0\t6.847077\tmonths_as_customer\n",
    "1\t6.852905\tage\n",
    "2\t1.038072\tpolicy_state\n",
    "3\t1.043977\tpolicy_deductable\n",
    "4\t1.036330\tpolicy_annual_premium\n",
    "5\t1.029258\tinsured_sex\n",
    "6\t1.046990\tinsured_education_level\n",
    "7\t1.016674\tinsured_occupation\n",
    "8\t1.049392\tinsured_hobbies\n",
    "9\t1.048658\tinsured_relationship\n",
    "10\t1.040356\tcapital-gains\n",
    "11\t1.042211\tcapital-loss\n",
    "12\t5.138442\tincident_type\n",
    "13\t1.047212\tcollision_type\n",
    "14\t1.195551\tincident_severity\n",
    "15\t1.084547\tauthorities_contacted\n",
    "16\t1.040581\tincident_state\n",
    "17\t1.029318\tincident_city\n",
    "18\t1.096675\tincident_hour_of_the_day\n",
    "19\t5.110588\tnumber_of_vehicles_involved\n",
    "20\t1.030105\tproperty_damage\n",
    "21\t1.026183\tbodily_injuries\n",
    "22\t1.043608\twitnesses\n",
    "23\t1.038953\tpolice_report_available\n",
    "24\t2.175649\tinjury_claim\n",
    "25\t2.253397\tproperty_claim\n",
    "26\t3.359149\tvehicle_claim\n",
    "27\t1.073774\tauto_make\n",
    "28\t1.065523\tauto_model\n",
    "29\t1.024664\tpolicy_bind_Day\n",
    "30\t1.039515\tpolicy_bind_Month\n",
    "31\t1.028024\tpolicy_bind_Year\n",
    "32\t1.967723\tincident_Day\n",
    "33\t1.971186\tincident_Month\n",
    "34\t1.032569\tcsl_per_person\n",
    "35\t1.040165\tVehicle_Age\n",
    "Now we are overcome with the multicollinearity issue as the VIF values are less than 10 in all the columns.\n",
    "\n",
    "y.value_counts()\n",
    "0    750\n",
    "1    246\n",
    "Name: fraud_reported, dtype: int64\n",
    "Here we can observe the data is not balanced, since it is classification problem we will balance the data using oversampling method.\n",
    "\n",
    "Oversampling\n",
    "# Oversampling the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "SM = SMOTE()\n",
    "x, y = SM.fit_resample(x,y)\n",
    "# Checking value count of target column\n",
    "y.value_counts()\n",
    "1    750\n",
    "0    750\n",
    "Name: fraud_reported, dtype: int64\n",
    "Now our data is balanced, so we can build our models.\n",
    "\n",
    "# Visualizing the data after oversampling\n",
    "sns.countplot(y,palette=\"Dark2\")\n",
    "<AxesSubplot:xlabel='fraud_reported', ylabel='count'>\n",
    "\n",
    "We can clearly observe the balanced data now.\n",
    "\n",
    "Modeling\n",
    "Finding best random state\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,200):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.30, random_state =i)\n",
    "    DTC = RandomForestClassifier()\n",
    "    DTC.fit(x_train, y_train)\n",
    "    pred = DTC.predict(x_test)\n",
    "    acc=accuracy_score(y_test, pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print(\"Best accuracy is \",maxAccu,\" on Random_state \",maxRS)\n",
    "Best accuracy is  0.9155555555555556  on Random_state  9\n",
    "We have got the best random state as 9 and maximum accuracy as 91.55%\n",
    "\n",
    "Creating train_test split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.30,random_state=maxRS)\n",
    "Classification Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "Random Forest Classifier\n",
    "# Checking accuracy for Random Forest Classifier\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predRFC = RFC.predict(x_test)\n",
    "rfc=accuracy_score(y_test, predRFC)\n",
    "print(rfc)\n",
    "print(confusion_matrix(y_test, predRFC))\n",
    "print(classification_report(y_test,predRFC))\n",
    "0.9\n",
    "[[197  24]\n",
    " [ 21 208]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.89      0.90       221\n",
    "           1       0.90      0.91      0.90       229\n",
    "\n",
    "    accuracy                           0.90       450\n",
    "   macro avg       0.90      0.90      0.90       450\n",
    "weighted avg       0.90      0.90      0.90       450\n",
    "\n",
    "The accuracy using Random Forest Classifier is 90%\n",
    "\n",
    "# Lets plot confusion matrix for RandomForestClassifier\n",
    "cm = confusion_matrix(y_test,predRFC)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for RandomForestClassifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "Support Vector Machine Classifier\n",
    "# Checking accuracy for Support Vector Machine Classifier\n",
    "svc = SVC()\n",
    "svc.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predsvc = svc.predict(x_test)\n",
    "sv=accuracy_score(y_test, predsvc)\n",
    "print(sv)\n",
    "print(confusion_matrix(y_test, predsvc))\n",
    "print(classification_report(y_test,predsvc))\n",
    "0.8822222222222222\n",
    "[[191  30]\n",
    " [ 23 206]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.86      0.88       221\n",
    "           1       0.87      0.90      0.89       229\n",
    "\n",
    "    accuracy                           0.88       450\n",
    "   macro avg       0.88      0.88      0.88       450\n",
    "weighted avg       0.88      0.88      0.88       450\n",
    "\n",
    "The accuracy score using Support Vector Machine classifier is 88.22%\n",
    "\n",
    "# Lets plot confusion matrix for Support Vector Machine Classifier\n",
    "cm = confusion_matrix(y_test,predsvc)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for Support Vector Machine Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "Gradient Boosting Classifier\n",
    "# Checking accuracy for Gradient Boosting Classifier\n",
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predGB = GB.predict(x_test)\n",
    "\n",
    "gb=accuracy_score(y_test, predGB)\n",
    "print(gb)\n",
    "print(confusion_matrix(y_test, predGB))\n",
    "print(classification_report(y_test,predGB))\n",
    "0.9111111111111111\n",
    "[[196  25]\n",
    " [ 15 214]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.89      0.91       221\n",
    "           1       0.90      0.93      0.91       229\n",
    "\n",
    "    accuracy                           0.91       450\n",
    "   macro avg       0.91      0.91      0.91       450\n",
    "weighted avg       0.91      0.91      0.91       450\n",
    "\n",
    "The accuracy using Gradient Boosting Classifier is 91.11%.\n",
    "\n",
    "# Lets plot confusion matrix for Gradient Boosting Classifier\n",
    "cm = confusion_matrix(y_test,predGB)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for Gradient Boosting Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "AdaBoost Classifier\n",
    "# Checking accuracy for AdaBoost Classifier\n",
    "ABC = AdaBoostClassifier()\n",
    "ABC.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predABC = ABC.predict(x_test)\n",
    "\n",
    "abc=accuracy_score(y_test, predABC)\n",
    "print(abc)\n",
    "print(confusion_matrix(y_test, predABC))\n",
    "print(classification_report(y_test,predABC))\n",
    "0.8555555555555555\n",
    "[[195  26]\n",
    " [ 39 190]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.88      0.86       221\n",
    "           1       0.88      0.83      0.85       229\n",
    "\n",
    "    accuracy                           0.86       450\n",
    "   macro avg       0.86      0.86      0.86       450\n",
    "weighted avg       0.86      0.86      0.86       450\n",
    "\n",
    "The accuracy using AdaBoost Classifier is 85.55%.\n",
    "\n",
    "# Lets plot confusion matrix for  AdaBoost Classifier\n",
    "cm = confusion_matrix(y_test,predABC)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for  AdaBoost Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "Bagging Classifier\n",
    "# Checking accuracy for BaggingClassifier\n",
    "BC = BaggingClassifier()\n",
    "BC.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predBC = BC.predict(x_test)\n",
    "bc=accuracy_score(y_test, predBC)\n",
    "print(bc)\n",
    "print(confusion_matrix(y_test, predBC))\n",
    "print(classification_report(y_test,predBC))\n",
    "0.8822222222222222\n",
    "[[194  27]\n",
    " [ 26 203]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.88      0.88       221\n",
    "           1       0.88      0.89      0.88       229\n",
    "\n",
    "    accuracy                           0.88       450\n",
    "   macro avg       0.88      0.88      0.88       450\n",
    "weighted avg       0.88      0.88      0.88       450\n",
    "\n",
    "The accuracy using Bagging classifier is 88.22%\n",
    "\n",
    "# Lets plot confusion matrix for  Bagging Classifier\n",
    "cm = confusion_matrix(y_test,predBC)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for  Bagging Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "Extra Trees Classifier\n",
    "# Checking accuracy for ExtraTreesClassifier\n",
    "XT = ExtraTreesClassifier()\n",
    "XT.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predXT = XT.predict(x_test)\n",
    "\n",
    "etc=accuracy_score(y_test, predXT)\n",
    "print(etc)\n",
    "print(confusion_matrix(y_test, predXT))\n",
    "print(classification_report(y_test,predXT))\n",
    "0.9288888888888889\n",
    "[[202  19]\n",
    " [ 13 216]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.91      0.93       221\n",
    "           1       0.92      0.94      0.93       229\n",
    "\n",
    "    accuracy                           0.93       450\n",
    "   macro avg       0.93      0.93      0.93       450\n",
    "weighted avg       0.93      0.93      0.93       450\n",
    "\n",
    "The accuracy using Extra Trees Classifier is 92.88%\n",
    "\n",
    "# Lets plot confusion matrix for  ExtraTreesClassifier\n",
    "cm = confusion_matrix(y_test,predXT)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for  ExtraTrees Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "XGB Classifier\n",
    "# Checking accuracy for XGBClassifier\n",
    "XGB = xgb(verbosity=0)\n",
    "XGB.fit(x_train,y_train)\n",
    "\n",
    "# Prediction\n",
    "predXGB = XGB.predict(x_test)\n",
    "xgb1=accuracy_score(y_test, predXGB)\n",
    "print(xgb1)\n",
    "print(confusion_matrix(y_test, predXGB))\n",
    "print(classification_report(y_test,predXGB))\n",
    "0.9088888888888889\n",
    "[[198  23]\n",
    " [ 18 211]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.90      0.91       221\n",
    "           1       0.90      0.92      0.91       229\n",
    "\n",
    "    accuracy                           0.91       450\n",
    "   macro avg       0.91      0.91      0.91       450\n",
    "weighted avg       0.91      0.91      0.91       450\n",
    "\n",
    "The accuracy using XGB classifier is 90.88%\n",
    "\n",
    "# Lets plot confusion matrix for  XGBClassifier\n",
    "cm = confusion_matrix(y_test,predXGB)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Dark2\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for  XGB Classifier')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able observe the true positive rate, false positive rate, true negative rate and false negative rate. And is plotted predicted value againt True values.\n",
    "\n",
    "Checking Cross Validation Score\n",
    "# cv score for Random Forest Classifier\n",
    "rf=cross_val_score(RFC,x,y,cv=5).mean()\n",
    "print(rf)\n",
    "0.876\n",
    "# cv score for Support Vector Machine Classifier\n",
    "sv_cv=cross_val_score(svc,x,y,cv=5).mean()\n",
    "print(sv_cv)\n",
    "0.858\n",
    "# cv score for Gradient Boosting Classifier\n",
    "gb_cv=cross_val_score(GB,x,y,cv=5).mean()\n",
    "print(gb_cv)\n",
    "0.8713333333333333\n",
    "# cv score for AdaBoosting Classifier\n",
    "ada_cv=cross_val_score(ABC,x,y,cv=5).mean()\n",
    "print(ada_cv)\n",
    "0.8386666666666667\n",
    "# cv score for Bagging Classifier\n",
    "bc_cv=cross_val_score(BC,x,y,cv=5).mean()\n",
    "print(bc_cv)\n",
    "0.876\n",
    "# cv score for Extra Trees Classifier\n",
    "ex_cv=cross_val_score(XT,x,y,cv=5).mean()\n",
    "print(ex_cv)\n",
    "0.9146666666666668\n",
    "# cv score for XGB Classifier\n",
    "xgb_cv=cross_val_score(XGB,x,y,cv=5).mean()\n",
    "print(xgb_cv)\n",
    "0.8766666666666667\n",
    "model_list=['Random Forest Classifier','Support Vector Machine Classifier','Gradient Boosting Classifier','AdaBoosting Classifier','Bagging Classifier','Extra Trees Classifier','XGB Classifier']\n",
    "accuracyscore=[rfc,sv,gb,abc,bc,etc,xgb1]\n",
    "crossval=[rf,sv_cv,gb_cv,ada_cv,bc_cv,ex_cv,xgb_cv]\n",
    "models=pd.DataFrame({})\n",
    "models[\"Classifier\"]=model_list\n",
    "models[\"Accuracy_score\"]=accuracyscore\n",
    "models[\"Cross Validation_Score\"]=crossval\n",
    "models\n",
    "Classifier\tAccuracy_score\tCross Validation_Score\n",
    "0\tRandom Forest Classifier\t0.900000\t0.876000\n",
    "1\tSupport Vector Machine Classifier\t0.882222\t0.858000\n",
    "2\tGradient Boosting Classifier\t0.911111\t0.871333\n",
    "3\tAdaBoosting Classifier\t0.855556\t0.838667\n",
    "4\tBagging Classifier\t0.882222\t0.876000\n",
    "5\tExtra Trees Classifier\t0.928889\t0.914667\n",
    "6\tXGB Classifier\t0.908889\t0.876667\n",
    "Above are the cross validation score for the models.\n",
    "\n",
    "From the difference between the accuracy score and the cross validation score we can conclude that ExtraTrees Classifer as our best fitting model whch is giving very less difference compare to other models.\n",
    "\n",
    "Hyper Parameter Tuning\n",
    "# ExtraTrees Classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'criterion' : ['gini','entropy'],\n",
    "              'max_features':['aoto','sqrt','log2'],\n",
    "              'max_depth' : [0, 10, 20],\n",
    "              'n_jobs' : [-2, -1, 1],\n",
    "              'n_estimators' : [50,100, 200, 300]}\n",
    "These are the parameters for ExtraTrees classifier.\n",
    "\n",
    "GCV=GridSearchCV(ExtraTreesClassifier(),parameters,cv=5)\n",
    "Running GridSearchCV for ExtraTrees Classifier.\n",
    "\n",
    "GCV.fit(x_train,y_train)\n",
    "GridSearchCV(cv=5, estimator=ExtraTreesClassifier(),\n",
    "             param_grid={'criterion': ['gini', 'entropy'],\n",
    "                         'max_depth': [0, 10, 20],\n",
    "                         'max_features': ['aoto', 'sqrt', 'log2'],\n",
    "                         'n_estimators': [50, 100, 200, 300],\n",
    "                         'n_jobs': [-2, -1, 1]})\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "GCV.best_params_\n",
    "{'criterion': 'gini',\n",
    " 'max_depth': 20,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 300,\n",
    " 'n_jobs': 1}\n",
    "These are the best parameters values that we have got for ExtraTrees classifier.\n",
    "\n",
    "FinalModel = ExtraTreesClassifier(criterion='gini', max_depth=20, max_features='log2', n_estimators=200, n_jobs=-2)\n",
    "FinalModel.fit(x_train, y_train)\n",
    "pred = FinalModel.predict(x_test)\n",
    "acc=accuracy_score(y_test,pred)\n",
    "print(acc*100)\n",
    "92.0\n",
    "After tuning the model we are getting 92% accuracy.\n",
    "\n",
    "# Lets plot confusion matrix for  FinalModel\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "\n",
    "x_axis_labels = [\"NO\",\"YES\"]\n",
    "y_axis_labels = [\"NO\",\"YES\"]\n",
    "\n",
    "f , ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cm, annot = True,linewidths=.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"ocean\",xticklabels=x_axis_labels,yticklabels=y_axis_labels)\n",
    "\n",
    "plt.xlabel(\"PREDICTED LABEL\")\n",
    "plt.ylabel(\"TRUE LABEL\")\n",
    "plt.title('Confusion Matrix for  Final Model')\n",
    "plt.show()\n",
    "\n",
    "With the help of confusion matrix we can able to see actual and predicted values.\n",
    "\n",
    "Plotting ROC and compare AUC for all the models used\n",
    "# Plotting for all the models used here\n",
    "from sklearn import datasets \n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import plot_roc_curve \n",
    "\n",
    "\n",
    "disp = plot_roc_curve(XT,x_test,y_test)     # ax_=Axes with confusion matrix\n",
    "plot_roc_curve(RFC, x_test, y_test, ax=disp.ax_)\n",
    "plot_roc_curve(svc, x_test, y_test, ax=disp.ax_)\n",
    "plot_roc_curve(GB, x_test, y_test, ax=disp.ax_)\n",
    "plot_roc_curve(ABC, x_test, y_test, ax=disp.ax_)\n",
    "plot_roc_curve(BC, x_test, y_test, ax=disp.ax_)\n",
    "plot_roc_curve(XGB, x_test, y_test, ax=disp.ax_)\n",
    "\n",
    "plt.legend(prop={'size':11}, loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "Here we can see the Area under curve for each model used here.\n",
    "\n",
    "Plotting ROC and Compare AUC for the best model\n",
    "# Let's check the Auc for the best model after hyper parameter tuning\n",
    "plot_roc_curve(FinalModel, x_test, y_test)\n",
    "plt.title(\"ROC for the best model\")\n",
    "plt.show()\n",
    "\n",
    "Here we have plotted the ROC curve for the final model and the AUC value for the best model is 98%.\n",
    "\n",
    "Saving The Model\n",
    "# Saving the model using .pkl\n",
    "import joblib\n",
    "joblib.dump(FinalModel,\"InsuranceclaimFraudDetection.pkl\")\n",
    "['InsuranceclaimFraudDetection.pkl']\n",
    "We have saved our model using joblib library.\n",
    "\n",
    "Predicting the saved model\n",
    "# Let's load the saved model and get the prediction\n",
    "\n",
    "# Loading the saved model\n",
    "model=joblib.load(\"InsuranceclaimFraudDetection.pkl\")\n",
    "\n",
    "#Prediction\n",
    "prediction = model.predict(x_test)\n",
    "prediction\n",
    "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
    "       1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
    "       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
    "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
    "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
    "       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
    "       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
    "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
    "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
    "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
    "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
    "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
    "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
    "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
    "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
    "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
    "       1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
    "       0, 0, 1, 1, 1, 0, 1, 0, 0, 1])\n",
    "These are the predicted fraud_reported of the customers.\n",
    "\n",
    "pd.DataFrame([model.predict(x_test)[:],y_test[:]],index=[\"Predicted\",\"Original\"]).T\n",
    "Predicted\tOriginal\n",
    "0\t1\t1\n",
    "1\t0\t0\n",
    "2\t1\t0\n",
    "3\t1\t1\n",
    "4\t1\t1\n",
    "...\t...\t...\n",
    "445\t0\t0\n",
    "446\t1\t1\n",
    "447\t0\t1\n",
    "448\t0\t0\n",
    "449\t1\t1\n",
    "450 rows Ã— 2 columns\n",
    "\n",
    "Here we can observe that actual and predicted fraud reports are almost same.\n",
    "\n",
    "Thank You\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
